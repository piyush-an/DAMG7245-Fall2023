{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Big-Data Systems and Intelligence Analytics","text":"<p>Welcome to the DAMG7245 course on Big Data Systems and Intelligent Analytics!</p> <p>We're excited to have you as part of this class, where we'll explore the fascinating world of big data and how it can be harnessed for intelligent analytics. This course will provide you with the opportunity to learn a hands-on approach to understanding how large-scale data sets are processed and how AI, ML and data science algorithms are adopted in the industry through case studies and labs. This project-based course focuses on enabling students with tools and frameworks primarily to build endto-end applications. The course is divided into three parts: Building the data pipeline for data science, Implementing data science algorithms, and Scaling and Deploying data science algorithms.</p> <p>If you have any questions or need assistance, please don't hesitate to reach out to the instructor or refer to the course materials. Let's embark on this exciting journey together!</p>"},{"location":"#course-information","title":"Course Information","text":"<ul> <li>Course Title: Big-Data Systems and Intelligence Analytics</li> <li>Course Number: DAMG 7245</li> <li>Term and Year: Fall 2023</li> <li>Credit Hour: 4</li> </ul>"},{"location":"#instructor-information","title":"Instructor Information","text":"<ul> <li> <p>Instructor: Srikanth Krishnamurthy</p> </li> <li> <p>Email: s.krishnamurthy@northeastern.edu</p> </li> <li> <p>Teaching Assistants (TAs):</p> <ul> <li>Piyush Anand: anand.pi@northeastern.edu</li> </ul> </li> </ul>"},{"location":"airflow/dag/","title":"Creating Airflow DAGs","text":"","tags":["DAG","BashOperator","PythonOperator"]},{"location":"airflow/dag/#introduction","title":"Introduction","text":"<p>In this tutorial, we will learn how to create and manage Directed Acyclic Graphs (DAGs) in Apache Airflow. Airflow is an open-source platform to programmatically author, schedule, and monitor workflows. DAGs in Airflow represent a collection of tasks with defined dependencies, allowing you to build and schedule complex data pipelines.</p>","tags":["DAG","BashOperator","PythonOperator"]},{"location":"airflow/dag/#prerequisites","title":"Prerequisites","text":"<p>Before we begin, make sure you have the airflow service up and running via docker.</p>","tags":["DAG","BashOperator","PythonOperator"]},{"location":"airflow/dag/#instructions","title":"Instructions","text":"","tags":["DAG","BashOperator","PythonOperator"]},{"location":"airflow/dag/#step-1-project-setup","title":"Step 1: Project Setup","text":"<p>First, set up a project directory to organize your Airflow DAGs and scripts. Here's a recommended project structure:</p> <pre><code>project_root/\n\u2502\n\u251c\u2500\u2500 dags/\n\u2502   \u2514\u2500\u2500 your_dag.py\n\u2502\n\u251c\u2500\u2500 scripts/\n\u2502   \u2514\u2500\u2500 your_script.py\n\u2502\n\u2514\u2500\u2500 ...\n</code></pre>","tags":["DAG","BashOperator","PythonOperator"]},{"location":"airflow/dag/#step-2-create-a-dag","title":"Step 2: Create a DAG","text":"<p>A DAG is a Python script that defines the structure of your workflow. Create a python file under the dag directory with the name <code>sandbx.py</code></p> <p>Here's a basic example of how to create a simple DAG:</p>","tags":["DAG","BashOperator","PythonOperator"]},{"location":"airflow/dag/#step-3-importing-required-modules","title":"Step 3: Importing Required Modules","text":"<p>In the first step of creating an Apache Airflow DAG, you need to import the necessary modules and libraries to build your workflow. These modules provide the tools and components required for defining and executing tasks within the DAG.</p> sandbox.py<pre><code>import os\nfrom airflow.models import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.utils.dates import days_ago\nfrom datetime import timedelta\n</code></pre> <p>The first step in writing a DAG (Directed Acyclic Graph) file for Apache Airflow involves creating a DAG object. In your provided code snippet, you've initiated a DAG named \"sandbox\" with various attributes and configurations. </p> sandbox.py<pre><code>dag = DAG(\n    dag_id=\"sandbox\",\n    schedule=\"0 0 * * *\",   # https://crontab.guru/\n    start_date=days_ago(0),\n    catchup=False,\n    dagrun_timeout=timedelta(minutes=60),\n    tags=[\"labs\", \"damg7245\"],\n    # params=user_input,\n)\n</code></pre> <p>Let's break down the code and explain each part:</p> <ul> <li>dag_id: This is a unique identifier for your DAG. In this case, it's named \"sandbox,\" but you should use a meaningful and descriptive name for your specific use case.</li> <li>schedule: This attribute defines the schedule or frequency at which the DAG should run. In your example, the schedule is set to \"0 0 * * *,\" which corresponds to a daily schedule at midnight. The format follows a cron-like pattern. You can use tools like crontab.guru to help generate the desired cron expression for your specific schedule requirements.</li> <li>start_date: This attribute sets the initial date when your DAG should start running. In your code, days_ago(0) is used to specify the current date and time as the start date.</li> <li>catchup: When set to False, as in your code, it means that the DAG won't \"catch up\" on any missed runs if the schedule's start date is in the past. If set to True, Airflow will execute runs for any missed intervals.</li> <li>dagrun_timeout: This attribute defines the maximum amount of time allowed for a DAG run to execute. In your code, it's set to a timeout of 60 minutes. If a DAG run exceeds this duration, it will be * **marked as failed.</li> <li>tags: Tags are labels or metadata that you can associate with your DAG. In your code, you've added two tags, \"labs\" and \"damg7245.\" Tags can be useful for categorizing and organizing your DAGs.</li> <li>params: This is commented out in your code but can be used to pass parameters to your DAG. If you have specific parameters or configurations that your DAG needs, you can provide them here.</li> </ul>","tags":["DAG","BashOperator","PythonOperator"]},{"location":"airflow/dag/#step-4-task-using-bashoperator","title":"Step 4: Task Using BashOperator","text":"<p>This task is implemented using a <code>BashOperator</code>, which allows you to execute a shell command as part of your workflow. </p> sandbox.py<pre><code>with dag:\n    hello_world = BashOperator(\n        task_id=\"hello_world\",\n        bash_command='echo \"Hello from airflow\"'\n    )\n</code></pre> <p>Here's what this specific task does:</p> <p>This command uses the echo command to output the text \"Hello from Airflow\" to the standard output (usually the terminal). In Airflow, this text will be logged and can be viewed in Airflow's logs or user interface.</p>","tags":["DAG","BashOperator","PythonOperator"]},{"location":"airflow/dag/#step-5-task-using-pythonoperator","title":"Step 5: Task Using PythonOperator","text":"<p>This task is implemented using a <code>BashOperator</code>, which allows you to execute a python command as part of your workflow. </p> <p>sandbox.py<pre><code>def print_keys(**kwargs):\n    print(\"-----------------------------\")\n    print(f\"Your Secret key is: {os.getenv('OPENAI_KEY')}\") # Donot print this anywhere, this is just for demo\n    print(\"-----------------------------\")\n\n    .\n    .\n    .\n\n    fetch_keys = PythonOperator(\n        task_id='fetch_keys',\n        python_callable=print_keys,\n        provide_context=True,\n        dag=dag,\n    )\n</code></pre> Here's what this specific task does:</p> <p>It prints the value of the environment variable OPENAI_KEY by using os.getenv('OPENAI_KEY'). This is intended to show the value of a secret key, but a comment indicates that it should not be printed anywhere, as it is sensitive information.</p> Warning <p>Never print sensitive variables into logs</p>","tags":["DAG","BashOperator","PythonOperator"]},{"location":"airflow/dag/#step-6-task-dependency","title":"Step 6: Task Dependency","text":"sandbox.py<pre><code>    # Flow\n    hello_world &gt;&gt; fetch_keys &gt;&gt; bye_world\n</code></pre> <p>The <code>hello_world</code>, <code>fetch_keys</code>, and <code>bye_world</code> tasks are linked together to form a workflow. The <code>&gt;&gt;</code> operator signifies task dependencies, meaning that fetch_keys should only execute if hello_world completes successfully, and bye_world should only execute after fetch_keys has successfully finished. This creates a sequential flow for your tasks.</p>","tags":["DAG","BashOperator","PythonOperator"]},{"location":"airflow/dag/#step-7-execute-the-dag","title":"Step 7: Execute the DAG","text":"<p>To execute the DAG, locate and click the \"Trigger DAG\" button. This will initiate a new run of the DAG. You can monitor the progress of the DAG run by navigating to the \"DAG Runs\" tab, where you can view the status of the most recent and historical runs. Logs and status updates will be available in real-time. To inspect the logs and outputs of specific tasks within the DAG, you can click on individual task instances within the \"DAG Runs\" tab.</p>","tags":["DAG","BashOperator","PythonOperator"]},{"location":"airflow/dag/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we've learned how to create and manage Airflow DAGs. You can now build complex workflows by adding more tasks and defining their dependencies. Airflow provides a powerful way to automate and monitor data pipelines.</p>","tags":["DAG","BashOperator","PythonOperator"]},{"location":"airflow/install/","title":"Running Airflow in Docker","text":"<p>In this section, we'll guide you through the process of running Apache Airflow using Docker and Docker Compose. Docker simplifies the deployment and management of Airflow by containerizing its components, making it easier to set up and maintain. Follow these steps to get started.</p>","tags":["Airflow","Docker"]},{"location":"airflow/install/#instruction","title":"Instruction","text":"Note <p>The following steps are from the official docs Running Airflow in Docker. Refer to this resource for detailed instructions and configurations</p>","tags":["Airflow","Docker"]},{"location":"airflow/install/#step-1-fetching-docker-composeyaml","title":"Step 1: Fetching <code>docker-compose.yaml</code>","text":"<p>To deploy Airflow on Docker Compose, you should fetch the <code>docker-compose.yaml</code> configuration file. This file defines the services and their configurations required to run Airflow in containers. You can obtain this file from the official Airflow repository on GitHub or create a custom one tailored to your needs.</p> <p>The official Airflow project provides a <code>docker-compose.yaml</code> file that you can use as a starting point. This configuration includes the Airflow web server, scheduler, worker, and database services. You can fetch it using <code>curl</code> or download it from the Airflow GitHub repository:</p> <pre><code>curl -LfO 'https://raw.githubusercontent.com/apache/airflow/main/docker-compose.yaml'\n</code></pre> <p>This file contains several service definitions:</p> <ul> <li> <p><code>airflow-scheduler</code>: The scheduler monitors all tasks and DAGs, then triggers the task instances once their dependencies are complete.</p> </li> <li> <p><code>airflow-webserver</code>: The webserver is available at http://localhost:8080.</p> </li> <li> <p><code>airflow-worker</code>: The worker that executes the tasks given by the scheduler.</p> </li> <li> <p><code>airflow-triggerer</code>: The triggerer runs an event loop for deferrable tasks.</p> </li> <li> <p><code>airflow-init</code>: The initialization service.</p> </li> <li> <p><code>postgres</code>: The database.</p> </li> <li> <p><code>redis</code>: The Redis broker that forwards messages from the scheduler to the worker.</p> </li> </ul> <p>Some directories in the container are mounted, which means that their contents are synchronized between your computer and the container.</p> <ul> <li> <p><code>./dags</code>: You can place your DAG files in this directory.</p> </li> <li> <p><code>./logs</code>: This directory contains logs generated from task execution and scheduler activities.</p> </li> <li> <p><code>./config</code>: Use this directory to store custom log parsers or configuration files like <code>airflow_local_settings.py</code> to configure cluster policies.</p> </li> <li> <p><code>./plugins</code>: You can store your custom Airflow plugins in this directory.</p> </li> </ul>","tags":["Airflow","Docker"]},{"location":"airflow/install/#step-2-initializing-environment","title":"Step 2: Initializing Environment","text":"<p>Before starting Airflow for the first time, you need to prepare your environment, i.e. create the necessary files, directories and initialize the database.</p> <p>1. Setting the Right Airflow User</p> <p>On Linux, the quick-start needs to know your host user id and needs to have group id set to 0. Otherwise the files created in dags, logs and plugins will be created with root user ownership. You have to make sure to configure them for the docker-compose:</p> <pre><code># Create a directory for Airflow and navigate into it\nmkdir airflow\ncd airflow\n\n# Create essential directories for Airflow\nmkdir -p ./dags ./logs ./plugins ./config\n# Navigate back to the previous directory\ncd ..\n\n# Create a .env file with the AIRFLOW_UID variable to store your host user ID\n# This step ensures that files created in dags, logs, and plugins have the correct ownership\necho -e \"AIRFLOW_UID=$(id -u)\" &gt; .env\n</code></pre> <p>2. Update the Environment File</p> <p>In the .env file, you'll add the AIRFLOW_PROJ_DIR variable, specifying the path to the Airflow project directory. This variable helps Airflow locate essential directories.</p> <p>.env<pre><code># Set the Airflow user's user ID\nAIRFLOW_UID=\n\n# Define the path to the Airflow project directory\nAIRFLOW_PROJ_DIR=./airflow\n</code></pre> 3. Update the docker-compose File</p> <p>open your docker-compose.yaml file and locate the appropriate sections where these parameters are defined. Update them as mentioned below</p> <pre><code>AIRFLOW__CORE__LOAD_EXAMPLES: 'false' # Chnaged to false\nAIRFLOW__CORE__ENABLE_XCOM_PICKLING: 'true' # Add this parameter\n</code></pre> <p>Following these steps ensures that your Airflow environment is set up correctly and ready for use with Docker Compose.</p>","tags":["Airflow","Docker"]},{"location":"airflow/install/#step-3-running-airflow","title":"Step 3: Running Airflow","text":"<p>Now you can start all services:</p> <pre><code>docker compose up\n</code></pre> <p>In a second terminal you can check the condition of the containers and make sure that no containers are in an unhealthy condition:</p> <pre><code>$ docker ps\nCONTAINER ID   IMAGE                  COMMAND                  CREATED          STATUS                    PORTS                              NAMES\n247ebe6cf87a   apache/airflow:2.7.2   \"/usr/bin/dumb-init \u2026\"   3 minutes ago    Up 3 minutes (healthy)    8080/tcp                           compose_airflow-worker_1\ned9b09fc84b1   apache/airflow:2.7.2   \"/usr/bin/dumb-init \u2026\"   3 minutes ago    Up 3 minutes (healthy)    8080/tcp                           compose_airflow-scheduler_1\n7cb1fb603a98   apache/airflow:2.7.2   \"/usr/bin/dumb-init \u2026\"   3 minutes ago    Up 3 minutes (healthy)    0.0.0.0:8080-&gt;8080/tcp             compose_airflow-webserver_1\n74f3bbe506eb   postgres:13            \"docker-entrypoint.s\u2026\"   18 minutes ago   Up 17 minutes (healthy)   5432/tcp                           compose_postgres_1\n0bd6576d23cb   redis:latest           \"docker-entrypoint.s\u2026\"   10 hours ago     Up 17 minutes (healthy)   0.0.0.0:6379-&gt;6379/tcp             compose_redis_1\n</code></pre>","tags":["Airflow","Docker"]},{"location":"airflow/install/#step-4-accessing-the-environment","title":"Step 4: Accessing the environment","text":"<p>After starting Airflow, you can interact with it in 3 ways:</p> <p>1. Accessing the web interface Once the cluster has started up, you can log in to the web interface and begin experimenting with DAGs.</p> <p>The webserver is available at: <code>http://localhost:8080</code>. The default account has the login <code>airflow</code> and the password <code>airflow</code></p> <p></p> <p>2. Sending requests to the REST API</p> <p>Basic username password authentication is currently supported for the REST API, which means you can use common tools to send requests to the API.</p> <p></p>","tags":["Airflow","Docker"]},{"location":"airflow/install/#step-5-cleaning-up","title":"Step 5: Cleaning up","text":"<p>To stop and delete containers, delete volumes with database data and download images, run:</p> <pre><code>docker compose down\n</code></pre>","tags":["Airflow","Docker"]},{"location":"airflow/install/#conclusion","title":"Conclusion","text":"<p>In this tutorial, you've gained a foundational understanding of Apache Airflow and how to set it up with Docker and Docker Compose. With key concepts, terminology, and initial configurations covered, you're now well-prepared to start orchestrating and automating workflows using Airflow. As you explore the possibilities, remember that this tutorial is just the beginning. The Airflow ecosystem offers extensive capabilities and customization options for more advanced use cases. For further learning and troubleshooting, the Airflow community and official documentation are valuable resources. Your Airflow journey has just begun \u2013 happy orchestrating!</p>","tags":["Airflow","Docker"]},{"location":"airflow/intro/","title":"Apache Airflow","text":"","tags":["Airflow"]},{"location":"airflow/intro/#introduction","title":"Introduction","text":"<p>Apache Airflow is a modern data pipelines and workflow automation require robust tools that can handle complex scheduling, dependency management, and error handling. Apache Airflow is a popular open-source solution that fulfills these requirements. It provides a unified platform to create, schedule, and monitor workflows while offering flexibility in choosing your preferred execution environment.</p> <p>Airflow allows you to define Directed Acyclic Graphs (DAGs) that represent your workflows, making it easy to visualize the dependencies between tasks and manage their execution. This orchestration tool is highly extensible and customizable, making it suitable for a wide range of use cases.</p>","tags":["Airflow"]},{"location":"airflow/intro/#key-terminology-in-apache-airflow","title":"Key Terminology in Apache Airflow","text":"<p>Apache Airflow comes with its own set of terms and concepts that are essential to understand as you delve into workflow orchestration. Here are some of the most important terms:</p>","tags":["Airflow"]},{"location":"airflow/intro/#1-directed-acyclic-graph-dag","title":"1. Directed Acyclic Graph (DAG)","text":"<p>A Directed Acyclic Graph (DAG) is the fundamental building block in Apache Airflow. It represents a workflow and consists of a collection of tasks and their dependencies. DAGs define the order in which tasks should be executed and ensure that there are no cycles in the execution path.</p>","tags":["Airflow"]},{"location":"airflow/intro/#2-operators","title":"2. Operators","text":"<p>Operators are atomic, self-contained units of work in a DAG. They define what needs to be done in a task. Airflow provides various built-in operators for different types of tasks, such as BashOperator (for running bash commands), PythonOperator (for executing Python functions), and more. You can also create custom operators tailored to your specific needs.</p>","tags":["Airflow"]},{"location":"airflow/intro/#3-task","title":"3. Task","text":"<p>A task is an instance of an operator in a DAG. It represents a single unit of work within a workflow. Tasks can have dependencies on other tasks, dictating the order in which they should be executed.</p>","tags":["Airflow"]},{"location":"airflow/intro/#4-workflow","title":"4. Workflow","text":"<p>A workflow is a sequence of tasks defined within a DAG. It outlines the specific sequence of steps needed to achieve a certain outcome. Workflows are used to automate and manage complex processes.</p>","tags":["Airflow"]},{"location":"airflow/intro/#5-scheduler","title":"5. Scheduler","text":"<p>The Airflow Scheduler is responsible for scheduling the execution of tasks within the DAGs. It determines when each task should run based on their dependencies and the specified schedule.</p>","tags":["Airflow"]},{"location":"airflow/intro/#6-sensor","title":"6. Sensor","text":"<p>A Sensor is a specialized type of operator that waits for a certain condition to be met before proceeding. Sensors are often used to monitor external systems or resources and trigger tasks when specific conditions are satisfied.</p>","tags":["Airflow"]},{"location":"airflow/intro/#7-trigger","title":"7. Trigger","text":"<p>A Trigger is an external event that starts the execution of a DAG or a specific task within a DAG. Triggers can be manual (user-initiated) or automated based on certain conditions.</p>","tags":["Airflow"]},{"location":"airflow/intro/#8-executor","title":"8. Executor","text":"<p>The Executor is responsible for running tasks on worker nodes. Airflow supports different executor options, such as the LocalExecutor, CeleryExecutor, and more, allowing you to choose the most suitable execution environment for your needs.</p>","tags":["Airflow"]},{"location":"airflow/intro/#9-web-ui","title":"9. Web UI","text":"<p>Airflow provides a web-based user interface that allows you to monitor and manage your DAGs and tasks. The web UI provides insights into the status of your workflows and facilitates interaction with Airflow.</p>","tags":["Airflow"]},{"location":"airflow/intro/#10-xcom","title":"10. XCom","text":"<p>XCom (short for Cross-Communication) is a system for sharing small amounts of data between tasks in a DAG. It allows tasks to exchange information and results, which can be useful in more complex workflows.</p>","tags":["Airflow"]},{"location":"airflow/intro/#conclusion","title":"Conclusion","text":"<p>These key terms are the building blocks of understanding Apache Airflow. As you proceed through this tutorial, you'll become more familiar with how these concepts come together to create efficient and automated workflows.</p>","tags":["Airflow"]},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2023/10/31/pathways-llm-large-language-model-app/","title":"Pathway's LLM (Large Language Model) App","text":"<p>This is referred from https://github.com/pathwaycom/llm-app</p>"},{"location":"blog/2023/10/31/pathways-llm-large-language-model-app/#reproducibility","title":"Reproducibility","text":"<p>The codebase underwent a few updates:</p> <ul> <li>Integration of Streamlit within a container.</li> <li>Utilization of Docker Compose to initiate both containers.</li> <li>Inclusion of a sample .env file.</li> </ul> <p>These changes have been implemented and are available at piyush-an/llm-app</p>"},{"location":"blog/2023/10/31/pathways-llm-large-language-model-app/#sample-query","title":"Sample Query","text":"<ol> <li> <p>How to use LLMs in Pathway? <pre><code>$ curl --data '{\"user\": \"user\", \"query\": \"How to use LLMs in Pathway?\"}' http://localhost:8080/\n\"LLMs (Louvain Local Modularity) can be used in Pathway by following these steps:\\n\\n1. Import the necessary modules:\\n   ```python\\n   from pathway.stdlib.graphs.louvain_communities import LLM\\n   ```\\n\\n2. Create an instance of the LLM\"\n</code></pre></p> </li> <li> <p>How to connect to Kafka in Pathway? <pre><code>$ curl --data '{\"user\": \"user\", \"query\": \"How to connect to Kafka in Pathway?\"}' http://localhost:8080/\n\"To connect to Kafka in Pathway, you can use the `pathway.io.kafka` package. This package provides the necessary connectors for reading and writing data from and to Kafka streams.\"\n</code></pre></p> </li> </ol>"},{"location":"blog/2023/10/31/pathways-llm-large-language-model-app/#observation","title":"Observation","text":"<ol> <li>The knowledge base is structured data stored in the form of a dictionary in the folder location <code>examples/data/pathway-docs</code>.</li> <li>The app does not use a vector database and offers real-time retrieval of data from the knowledge base.</li> <li>The knowledge base can be updated in real time to add more files.</li> <li>Users have the option to utilize the Hugging Face model in place of OpenAI.</li> <li>Other available options for the knowledge base include PDF, HTML, DOCX, PPTX stored locally or on S3, and a SQL database.</li> </ol>"},{"location":"blog/2023/10/11/how-to-scrape-sec-form-data-using-beautifulsoup-in-python/","title":"How to Scrape SEC Form Data Using BeautifulSoup in Python","text":""},{"location":"blog/2023/10/11/how-to-scrape-sec-form-data-using-beautifulsoup-in-python/#getting-started-with-sec-form-data-scraping","title":"Getting Started with SEC Form Data Scraping","text":"<p>In the ever-evolving world of finance, access to timely and accurate data is a critical asset for investors, analysts, and researchers. The U.S. Securities and Exchange Commission (SEC) is a treasure trove of such data, with companies filing various financial reports and disclosures through SEC forms. Extracting and analyzing this data can provide valuable insights for decision-making and research.</p> <p>In this section, we'll embark on a journey to explore the process of scraping SEC form data using Python and the BeautifulSoup library. Whether you're a financial analyst seeking the latest corporate filings, a student conducting research, or just a curious data enthusiast, this guide will walk you through the fundamentals of web scraping and data extraction from the SEC's Electronic Data Gathering, Analysis, and Retrieval (EDGAR) database.</p> <p>By the end of this section, you'll have a solid foundation to navigate SEC form pages, retrieve financial information, and be well on your way to harnessing the power of web scraping for financial analysis.</p>"},{"location":"blog/2023/10/11/how-to-scrape-sec-form-data-using-beautifulsoup-in-python/#installing-beautifulsoup","title":"Installing BeautifulSoup","text":"<p>Before we dive into the world of web scraping and SEC form data extraction, we need to ensure that we have the necessary tools at our disposal. One of the key libraries we'll be using for this task is BeautifulSoup (often abbreviated as BS4).</p> <p>BeautifulSoup is a Python library that makes it easy to parse and navigate HTML and XML documents. It's particularly useful for web scraping because it allows you to search and extract specific data from web pages with ease.</p> <p>To get started with BeautifulSoup, we first need to install it. You can do this using pip, the Python package manager. If you're using a virtual environment for your project (which is recommended for managing dependencies), make sure you activate the environment before running the installation command.</p> <p>To install BeautifulSoup, open your command line or terminal and run the following command:</p> <p><pre><code>pip install beautifulsoup4\n</code></pre> This command will download and install BeautifulSoup and its dependencies. Once the installation is complete, you're ready to start working with BeautifulSoup to scrape SEC form data.</p>"},{"location":"blog/2023/10/11/how-to-scrape-sec-form-data-using-beautifulsoup-in-python/#making-http-requests-to-sec-forms-page","title":"Making HTTP Requests to SEC Forms Page","text":"<p>Now that we have BeautifulSoup installed, we're ready to begin the web scraping process. The first step in scraping SEC form data is to access the webpage containing the information we need. To do this, we'll use the <code>requests</code> library in Python.</p> <p>In the code snippet below, we start by importing the necessary libraries: <code>requests</code> for making HTTP requests, <code>pandas</code> for data manipulation, and <code>BeautifulSoup</code> for parsing HTML. These libraries are crucial tools in our web scraping toolkit.</p> <pre><code>import requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n</code></pre> <p>Next, we define the URL of the SEC Forms webpage that we want to scrape. In this example, we'll use the following URL:</p> <pre><code>url = 'https://www.sec.gov/forms'\n</code></pre> <p>Now, it's time to send an HTTP GET request to this URL. The requests.get() method is used for this purpose, as shown in the code snippet below:</p> <pre><code># Send an HTTP GET request to the URL\nresponse = requests.get(url)\n</code></pre> <p>Here, the requests.get(url) line sends an HTTP GET request to the specified URL, and the response from the server is stored in the response variable. This response object contains the content of the webpage, which we will later parse using BeautifulSoup.</p>"},{"location":"blog/2023/10/11/how-to-scrape-sec-form-data-using-beautifulsoup-in-python/#parsing-html-content-and-locating-the-html-table","title":"Parsing HTML Content and Locating the HTML Table","text":"<p>With the HTTP GET request successfully made to the SEC Forms webpage, we can now proceed to parse the HTML content of the page. To do this, we'll use BeautifulSoup, the Python library designed for precisely this purpose.</p> <p>In the code snippet below, we initiate the parsing process:</p> <pre><code>soup = BeautifulSoup(response.text, 'html.parser')\n</code></pre> <p>Here, we create a BeautifulSoup object named soup by passing two arguments. The first argument, response.text, is the content of the HTTP response that we received from the SEC Forms webpage. The second argument, <code>html.parser</code>, specifies the parser we want to use for parsing the HTML content.</p> <p>With the HTML content successfully parsed, the next step is to locate the HTML table that contains the SEC form data. This table is where the form data is organized in a structured manner.</p> <p>In the following code snippet, we search for the first <code>table</code> element within the parsed HTML:</p> <pre><code>table_element = soup.find('table')\n</code></pre> <p>The <code>soup.find('table')</code> line utilizes BeautifulSoup's find() method to search for the first <code>table</code> element within the parsed HTML. This element corresponds to the table that holds the SEC form data.</p> <p>Once we have located the table, the subsequent task is to find and retrieve the individual rows within the table. Each row corresponds to a different SEC form entry.</p> <p>In the code below, we use BeautifulSoup to find all the <code>tr</code> (table row) elements within the table:</p> <p><pre><code>rows = table_element.find_all('tr')\n</code></pre> The <code>table_element.find_all('tr')</code> line uses the <code>find_all()</code> method to collect all the <code>tr</code> elements, effectively creating a list of rows that we can iterate through and extract data from.</p>"},{"location":"blog/2023/10/11/how-to-scrape-sec-form-data-using-beautifulsoup-in-python/#extracting-data-and-creating-a-dataframe","title":"Extracting Data and Creating a DataFrame","text":"<p>Now that we've successfully parsed the HTML content of the SEC Forms webpage and located the table, the next step is to extract relevant data from the individual SEC form entries. In this section, we'll explore how the provided code accomplishes this task.</p> <p>We've initiated an empty list named <code>data</code> to store the extracted data:</p> <pre><code># Initialize an empty list to store extracted data\ndata = []\n\n# Loop through each row in the table, skipping the header row (index 0)\nfor row in rows[1:]:\n\n  # Extract the form number from the row\n  form_no = row.find('span', class_='show-for-small').find_next_sibling(string=True).strip()\n\n  # Extract the form title from the row\n  form_title = row.find('td', class_='display-title-content views-field views-field-field-display-title').find('a').get_text()\n\n  # Construct the full form URL by appending the base URL to the 'href' attribute\n  form_url = f\"https://www.sec.gov{row.find('td', class_='display-title-content views-field views-field-field-display-title').find('a')['href']}\"\n\n  # Extract the last updated date from the 'datetime' attribute\n  last_updated = row.find('time')['datetime']\n\n  # Send an HTTP GET request to check if the form URL is valid\n  response = requests.get(form_url)\n\n  # Check the HTTP response status code to determine if the link is valid\n  if response.status_code == 200:\n    link_valid = 'Y'  # Valid link\n  else:\n    link_valid = 'N'  # Invalid link\n\n  # Append the extracted data as a list to the 'data' list\n  data.append([form_no, form_title, form_url, last_updated, link_valid])\n\n# Create a DataFrame from the list of extracted values with defined column names\ndf = pd.DataFrame(data, columns=['Form No', 'Form Title', 'Form URL', 'Last Updated', 'Link Valid'])\n\n# Print the DataFrame to display the extracted SEC form data\ndf\n</code></pre> <p>Within this loop, we extract various pieces of information from each SEC form entry. Here's what we extract and how we do it:</p> <ul> <li> <p>form_no: We find the form number by locating a <code>&lt;span&gt;</code> element with the class 'show-for-small' and then navigating to the next sibling containing the form number text.</p> </li> <li> <p>form_title: We locate the form title by finding a <code>&lt;td&gt;</code> element with the class 'display-title-content' and a nested <code>&lt;a&gt;</code> element, from which we extract the text.</p> </li> <li> <p>form_url: We construct the form's URL by combining the SEC's base URL with the 'href' attribute of the anchor element.</p> </li> <li> <p>last_updated: We extract the 'datetime' attribute of the 'time' element, which provides the last update date of the form.</p> </li> <li> <p>link_valid: We check the validity of the form's URL by sending another HTTP GET request to the form's URL and verifying if the response status code is 200 (indicating a successful request). We store 'Y' if the link is valid and 'N' if it's not.</p> </li> </ul> <p>We append these extracted values as lists to the <code>data</code> list.</p> <p>After extracting the data, we use the <code>pandas</code> library to create a DataFrame named <code>df</code>. This DataFrame stores the extracted values and has the following columns: 'Form No', 'Form Title', 'Form URL', 'Last Updated', and 'Link Valid'.</p> <p>Finally, we print the DataFrame to display the extracted SEC form data in a structured and tabular format.</p> index Form No Form Title Form URL Last Updated Link Valid 0 Examination Brochure: Information about Examinations (PDF) https://www.sec.gov/files/exam-brochure.pdf 2023-01-31 Y 1 Transferred Federal Employees https://www.sec.gov/ohr/transferred-federal-employees-page-link 2016-05-24 Y 2 Term Employees https://www.sec.gov/ohr/term-employees-page-link 2016-05-24 Y 3 1 Application for registration or exemption from registration as a national securities exchange (PDF) https://www.sec.gov/files/form1.pdf 1999-02-01 Y 4 1-A Regulation A Offering Statement (PDF) https://www.sec.gov/files/form1-a.pdf 2021-09-27 Y"},{"location":"blog/2023/10/11/how-to-scrape-sec-form-data-using-beautifulsoup-in-python/#conclusion","title":"Conclusion","text":"<p>Congratulations! You've learned how to scrape SEC form data using Python and BeautifulSoup. The code provided in this blog post extracts valuable information from the SEC Forms webpage, including form numbers, titles, URLs, last updated dates, and link validity.</p> <p>Now, if you're eager to put this knowledge into practice and run the code yourself, I invite you to refer to our accompanying Python notebook. You can access the notebook at Github.</p> <p>In the Python notebook, you'll find the complete code and be able to execute it step by step. Feel free to modify and adapt it to your specific needs, whether you're a financial analyst seeking data for analysis, a student conducting research, or anyone interested in exploring the world of web scraping.</p> <p>Remember to stay mindful of ethical web scraping practices, respect the website's terms of use, and always handle data with care. Happy scraping, and may your exploration of SEC form data be insightful and productive!</p> <p>Scrape_SEC_forms.ipynb</p>"},{"location":"blog/2023/10/20/vector-database/","title":"Vector Database","text":"<p>A vector database is a specialized type of database designed for the storage, retrieval, and manipulation of vector data. Vector data consists of data points represented as vectors in multi-dimensional space. These databases are particularly useful for applications that require similarity search, recommendation systems, machine learning, and data analysis, where relationships and similarities between data points are crucial. Traditional relational databases are often not well-suited for handling high-dimensional data and complex similarity queries, making vector databases a valuable solution.</p> <p>Key features and characteristics of vector databases include:</p> <ol> <li> <p>Vector Storage: They efficiently store vector data, often using specialized data structures and indexing methods for fast retrieval.</p> </li> <li> <p>Similarity Search: Vector databases support similarity search operations to find data points that are most similar to a given query vector.</p> </li> <li> <p>Scalability: They are designed for horizontal scalability, making them suitable for large datasets and high query loads.</p> </li> <li> <p>Real-time Processing: Vector databases are optimized for real-time or near-real-time data retrieval and analysis.</p> </li> <li> <p>Support for High-Dimensional Data: They can handle data with a high number of dimensions, making them suitable for machine learning and deep learning applications.</p> </li> </ol>"},{"location":"blog/2023/10/20/vector-database/#pinecone-overview","title":"Pinecone Overview","text":"<p>Pinecone makes it easy to provide long-term memory for high-performance AI applications. It\u2019s a managed, cloud-native vector database with a simple API and no infrastructure hassles. Pinecone serves fresh, filtered query results with low latency at the scale of billions of vectors.</p>"},{"location":"blog/2023/10/20/vector-database/#references","title":"References:","text":"<ol> <li>Pinecone Overview</li> <li>What is a Vector Database?</li> <li>Pinecone Quickstart</li> <li>Pinecone Quick-Tour - Github</li> <li>Pinecone User Guide - YouTube</li> </ol>"},{"location":"container/build_fastapi/","title":"Dockerize FastAPI Application","text":"","tags":["FastAPI","Docker"]},{"location":"container/build_fastapi/#introduction","title":"Introduction","text":"<p>This documentation provides a step-by-step guide on how to Dockerize a FastAPI application. Docker is a powerful tool that allows you to package your application and its dependencies into a container, ensuring consistent and reproducible deployments across different environments. By following this guide, you'll be able to containerize your FastAPI application, making it more portable and easier to manage.</p>","tags":["FastAPI","Docker"]},{"location":"container/build_fastapi/#tutorial","title":"Tutorial","text":"","tags":["FastAPI","Docker"]},{"location":"container/build_fastapi/#step-1-exisitng-code","title":"Step 1: Exisitng code","text":"<p>Begin by moving to project directory for your FastAPI application. Ensure that your FastAPI application is working correctly in this directory before proceeding.</p> <pre><code>.\n\u251c\u2500\u2500 Pipfile\n\u251c\u2500\u2500 Pipfile.lock\n\u2514\u2500\u2500 main.py\n</code></pre>","tags":["FastAPI","Docker"]},{"location":"container/build_fastapi/#step-2-create-dockerfile","title":"Step 2: Create Dockerfile","text":"<p>A Dockerfile is used to define the configuration for building a Docker image. Create a file named <code>Dockerfile</code> (with no file extension) in your project directory. Here is a sample Dockerfile for a FastAPI application:</p> Dockerfile<pre><code># Pull the base docker image of python with tag 3.10.12\nFROM python:3.10.12\n\n# Install Pipenv\nRUN pip install pipenv\n\n# Change the working dir inside the container - cd /app\nWORKDIR /app\n\n# Copy main.py as source cod and req.txt as dependency\nCOPY main.py ./\n\nCOPY Pipfile Pipfile.lock ./\n# Install the dependency\nRUN pipenv install --system --deploy --ignore-pipfile\n\n### Container Env same as local at this point\nEXPOSE 8000\n\nCMD [\"gunicorn\" ,\"-w\", \"4\", \"-k\", \"uvicorn.workers.UvicornWorker\" , \"--bind\", \"0.0.0.0:8000\", \"main:app\"]\n# gunicorn command to run the service with 4 worker nodes binding localhost/0.0.0.0 on port 8000 refering app inside the main.py\n</code></pre>","tags":["FastAPI","Docker"]},{"location":"container/build_fastapi/#step-3-building-the-docker-image","title":"Step 3: Building the Docker Image","text":"<p>To create a Docker image for your FastAPI application, use the docker build command. Navigate to your project directory and run the following command:</p> <pre><code>docker build -t fastapi_demo:v1 .\n</code></pre> <p>Here's a breakdown of the command:</p> <ul> <li> <p><code>docker build</code>: This command is used to build a Docker image.</p> </li> <li> <p><code>-t fastapi_demo:v1</code>: The <code>-t</code> flag allows you to specify a name and optionally a tag for your image. In this example, we're naming the image <code>fastapi_demo</code> and giving it the tag <code>v1</code>. You can choose your own image name and tag.</p> </li> <li> <p><code>.</code>: The dot (<code>.</code>) at the end of the command specifies the build context, which is the current directory. This tells Docker to look for the <code>Dockerfile</code> in the current directory and use it to build the image.</p> </li> </ul> <p>The Docker image will be built using the instructions in your Dockerfile. You'll see output in the terminal as each step is executed. If there are any errors during the build process, Docker will report them, and you can address them accordingly</p> stdout<pre><code>[+] Building 1.0s (12/12) FINISHED                                                                                                                                                              docker:default\n =&gt; [internal] load build definition from Dockerfile                                                                                                                                                      0.0s\n =&gt; =&gt; transferring dockerfile: 701B                                                                                                                                                                      0.0s\n =&gt; [internal] load .dockerignore                                                                                                                                                                         0.0s\n =&gt; =&gt; transferring context: 2B                                                                                                                                                                           0.0s\n =&gt; [internal] load metadata for docker.io/library/python:3.10.12                                                                                                                                         0.9s\n =&gt; [auth] library/python:pull token for registry-1.docker.io                                                                                                                                             0.0s\n =&gt; [1/6] FROM docker.io/library/python:3.10.12@sha256:bac3a0e0d16125977e351c861e2f4b12ecafeaa6f72431dc970d0b9155103232                                                                                   0.0s\n =&gt; [internal] load build context                                                                                                                                                                         0.0s\n =&gt; =&gt; transferring context: 89B                                                                                                                                                                          0.0s\n =&gt; CACHED [2/6] RUN pip install pipenv                                                                                                                                                                   0.0s\n =&gt; CACHED [3/6] WORKDIR /app                                                                                                                                                                             0.0s\n =&gt; CACHED [4/6] COPY main.py ./                                                                                                                                                                          0.0s\n =&gt; CACHED [5/6] COPY Pipfile Pipfile.lock ./                                                                                                                                                             0.0s\n =&gt; CACHED [6/6] RUN pipenv install --system --deploy --ignore-pipfile                                                                                                                                    0.0s\n =&gt; exporting to image                                                                                                                                                                                    0.0s\n =&gt; =&gt; exporting layers                                                                                                                                                                                   0.0s\n =&gt; =&gt; writing image sha256:c6965620db94b6c47264e25a8ccdbb069d93c1bace54cc85f0b5392979c2fee3                                                                                                              0.0s\n =&gt; =&gt; naming to docker.io/library/fastapi_demo:v1                                                                                                                                                        0.0s\n\nWhat's Next?\n  View a summary of image vulnerabilities and recommendations \u2192 docker scout quickview\n</code></pre>","tags":["FastAPI","Docker"]},{"location":"container/build_fastapi/#step-4-verify-the-built-image","title":"Step 4: Verify the Built Image","text":"<p>Once the build process is complete, you can verify that the image was created successfully by running:</p> <pre><code>docker images\n</code></pre> <p>You should see your newly created image listed with the name and tag you specified (fastapi_demo:v1 in this example).</p> stdout<pre><code>REPOSITORY                     TAG       IMAGE ID       CREATED        SIZE\nfastapi_demo                   v1        c6965620db94   19 hours ago   9.07GB\n</code></pre>","tags":["FastAPI","Docker"]},{"location":"container/build_fastapi/#step-5-running-the-docker-image","title":"Step 5: Running the Docker Image","text":"<p>Now that you have built your Docker image, you can run it as a container. To do this, use the docker run command. In your terminal, navigate to the project directory, and execute the following command:</p> <pre><code>docker run -p 8090:8000 fastapi_demo:v1\n</code></pre> <p>Here's what each part of the command does:</p> <ul> <li><code>docker run</code>: This command starts a Docker container based on a Docker image.</li> <li><code>-p 8090:8000</code>: The <code>-p</code> flag is used to map ports between the host machine and the container. In this example, it maps port <code>8090</code> on your host to port <code>8000</code> in the container. You can adjust the host port (left side) as needed.</li> <li><code>fastapi_demo:v1</code>: This specifies the Docker image you want to run. Replace fastapi_demo:v1 with the name and tag of your Docker image.</li> </ul>","tags":["FastAPI","Docker"]},{"location":"container/build_fastapi/#step-6-access-your-fastapi-application","title":"Step 6: Access Your FastAPI Application","text":"<p>After running the command, your FastAPI application should be up and running inside a Docker container. You can access it by opening a web browser and navigating to: <code>http://localhost:8090/docs</code></p> <p></p>","tags":["FastAPI","Docker"]},{"location":"container/build_fastapi/#step-7-stopping-the-container","title":"Step 7: Stopping the Container","text":"<p>To stop the Docker container, you can press Ctrl+C in the terminal where it is running, or you can open a new terminal window and run the following command: <pre><code>docker stop &lt;container_id_or_name&gt;\n</code></pre></p> <p>Replace <code>container_id_or_name</code> with the actual container ID or name, which you can find using the docker ps command.</p> <pre><code>$ docker ps\nCONTAINER ID   IMAGE                          COMMAND                  CREATED         STATUS                 PORTS                                              NAMES\nfb83d5eb2bec   fastapi_demo:v1                \"gunicorn -w 4 -k uv\u2026\"   6 minutes ago   Up 6 minutes           0.0.0.0:8090-&gt;8000/tcp                             stoic_dijkstra\n</code></pre> <pre><code>docker stop fb83d5eb2bec\n</code></pre>","tags":["FastAPI","Docker"]},{"location":"container/build_fastapi/#conclusion","title":"Conclusion","text":"<p>In this guide, you've learned how to Dockerize your FastAPI application, creating a Docker image and running it as a container. Dockerization simplifies deployment, enhances scalability, and streamlines the development process by encapsulating your application and its environment. With your FastAPI application now containerized, you have the flexibility to deploy it in various environments with confidence. This marks the beginning of a more efficient and maintainable development and deployment workflow for your FastAPI projects. Feel free to explore further Docker features and optimizations to suit your specific application needs.</p>","tags":["FastAPI","Docker"]},{"location":"container/build_streamlit/","title":"Dockerize Streamlit Application","text":"","tags":["Streamlit","Docker"]},{"location":"container/build_streamlit/#introduction","title":"Introduction","text":"<p>This documentation provides a step-by-step guide on how to Dockerize a Streamlit application. Docker is a powerful tool that allows you to package your application and its dependencies into a container, ensuring consistent and reproducible deployments across different environments. By following this guide, you'll be able to containerize your Streamlit application, making it more portable and easier to manage.</p>","tags":["Streamlit","Docker"]},{"location":"container/build_streamlit/#tutorial","title":"Tutorial","text":"","tags":["Streamlit","Docker"]},{"location":"container/build_streamlit/#step-1-exisitng-code","title":"Step 1: Exisitng code","text":"<p>Begin by moving to project directory for your Streamlit application. Ensure that your Streamlit application is working correctly in this directory before proceeding.</p> <pre><code>.\n\u251c\u2500\u2500 Pipfile\n\u251c\u2500\u2500 Pipfile.lock\n\u2514\u2500\u2500 main.py\n</code></pre>","tags":["Streamlit","Docker"]},{"location":"container/build_streamlit/#step-2-create-dockerfile","title":"Step 2: Create Dockerfile","text":"<p>A Dockerfile is used to define the configuration for building a Docker image. Create a file named <code>Dockerfile</code> (with no file extension) in your project directory. Here is a sample Dockerfile for a Streamlit application:</p> Dockerfile<pre><code># Pull the base docker image of python with tag 3.10.12\nFROM python:3.10.12\n\n# Install Pipenv\nRUN pip install pipenv\n\n# Change the working dir inside the container - cd /app\nWORKDIR /app\n\n# Copy main.py as source cod and req.txt as dependency\nCOPY . ./\n\n# Install the dependency\nRUN pipenv install --system --deploy --ignore-pipfile\n\n### Container Env same as local at this point\nEXPOSE 8090\n\nCMD [\"streamlit\", \"run\", \"main.py\", \"--server.port\", \"8090\"]\n</code></pre>","tags":["Streamlit","Docker"]},{"location":"container/build_streamlit/#step-3-building-the-docker-image","title":"Step 3: Building the Docker Image","text":"<p>To create a Docker image for your Streamlit application, use the docker build command. Navigate to your project directory and run the following command:</p> <pre><code>docker build -t streamlit_demo:v1 .\n</code></pre> <p>Here's a breakdown of the command:</p> <ul> <li> <p><code>docker build</code>: This command is used to build a Docker image.</p> </li> <li> <p><code>-t streamlit_demo:v1</code>: The <code>-t</code> flag allows you to specify a name and optionally a tag for your image. In this example, we're naming the image <code>streamlit_demo</code> and giving it the tag <code>v1</code>. You can choose your own image name and tag.</p> </li> <li> <p><code>`.</code>: The dot (<code>.</code>) at the end of the command specifies the build context, which is the current directory. This tells Docker to look for the <code>Dockerfile</code> in the current directory and use it to build the image.</p> </li> </ul> <p>The Docker image will be built using the instructions in your Dockerfile. You'll see output in the terminal as each step is executed. If there are any errors during the build process, Docker will report them, and you can address them accordingly</p>","tags":["Streamlit","Docker"]},{"location":"container/build_streamlit/#step-4-verify-the-built-image","title":"Step 4: Verify the Built Image","text":"<p>Once the build process is complete, you can verify that the image was created successfully by running:</p> <pre><code>docker images\n</code></pre> <p>You should see your newly created image listed with the name and tag you specified (streamlit_demo:v1 in this example).</p> stdout<pre><code>REPOSITORY                     TAG       IMAGE ID       CREATED        SIZE\nstreamlit_demo                   v1        9e56e49d32e5   19 hours ago   1.5GB\n</code></pre>","tags":["Streamlit","Docker"]},{"location":"container/build_streamlit/#step-5-running-the-docker-image","title":"Step 5: Running the Docker Image","text":"<p>Now that you have built your Docker image, you can run it as a container. To do this, use the docker run command. In your terminal, navigate to the project directory, and execute the following command:</p> <pre><code>docker run -p 8090:8090 streamlit_demo:v1\n</code></pre> <p>Here's what each part of the command does:</p> <ul> <li><code>**docker run**</code>: This command starts a Docker container based on a Docker image.</li> <li><code>**-p 8090:8090**</code>: The <code>-p</code> flag is used to map ports between the host machine and the container. In this example, it maps port <code>8090</code> on your host to port <code>8090</code> in the container. You can adjust the host port (left side) as needed.</li> <li><code>**streamlit_demo:v1**</code>: This specifies the Docker image you want to run. Replace streamlit_demo:v1 with the name and tag of your Docker image.</li> </ul>","tags":["Streamlit","Docker"]},{"location":"container/build_streamlit/#step-6-access-your-streamlit-application","title":"Step 6: Access Your Streamlit Application","text":"<p>After running the command, your Streamlit application should be up and running inside a Docker container. You can access it by opening a web browser and navigating to: <code>http://localhost:8090</code></p>","tags":["Streamlit","Docker"]},{"location":"container/build_streamlit/#step-7-stopping-the-container","title":"Step 7: Stopping the Container","text":"<p>To stop the Docker container, you can press Ctrl+C in the terminal where it is running, or you can open a new terminal window and run the following command: <pre><code>docker stop &lt;container_id_or_name&gt;\n</code></pre></p> <p>Replace <code>container_id_or_name</code> with the actual container ID or name, which you can find using the docker ps command.</p> <pre><code>$ docker ps\nCONTAINER ID   IMAGE                          COMMAND                  CREATED         STATUS                 PORTS                                              NAMES\n6f641cdaaf76   streamlit_demo:v1                \"streamlit run main.\u2026\"   6 minutes ago   Up 6 minutes           0.0.0.0:8090-&gt;8090/tcp                             stoic_dijkstra\n</code></pre> <pre><code>docker stop 6f641cdaaf76\n</code></pre>","tags":["Streamlit","Docker"]},{"location":"container/build_streamlit/#conclusion","title":"Conclusion","text":"<p>In this guide, you've learned how to Dockerize your Streamlit application, creating a Docker image and running it as a container. Dockerization simplifies deployment, enhances scalability, and streamlines the development process by encapsulating your application and its environment. With your Streamlit application now containerized, you have the flexibility to deploy it in various environments with confidence. This marks the beginning of a more efficient and maintainable development and deployment workflow for your Streamlit projects. Feel free to explore further Docker features and optimizations to suit your specific application needs.</p>","tags":["Streamlit","Docker"]},{"location":"container/install/","title":"Installing Docker Desktop on Windows and macOS","text":"<p>Docker Desktop is a convenient way to run Docker containers on your Windows or macOS machine. It provides an easy-to-use interface for managing containers and orchestrating your development environment. Follow these steps to install Docker Desktop on your system.</p>","tags":["Docker Desktop","MacOS","Windows"]},{"location":"container/install/#windows-installation","title":"Windows Installation","text":"","tags":["Docker Desktop","MacOS","Windows"]},{"location":"container/install/#prerequisites","title":"Prerequisites","text":"<p>Before installing Docker Desktop on Windows, ensure that your system meets the following requirements:</p> <ul> <li>Windows 10 Pro, Enterprise, or Education (64-bit)</li> <li>Virtualization must be enabled in the BIOS/UEFI settings.</li> <li>Hyper-V must be enabled (usually enabled by default in Windows 10 Pro).</li> </ul>","tags":["Docker Desktop","MacOS","Windows"]},{"location":"container/install/#installation-steps","title":"Installation Steps","text":"<ol> <li> <p>Go to the Docker Desktop for Windows download page.</p> </li> <li> <p>Download the Docker Desktop for Windows installer.</p> </li> <li> <p>Double-click the installer to run it. You may be prompted for administrator permissions.</p> </li> <li> <p>Follow the installation wizard's instructions to install Docker Desktop.</p> </li> <li> <p>During the installation, you will be asked if you want to use Windows containers or Linux containers. You can select either option or choose to use both.</p> </li> <li> <p>Once the installation is complete, Docker Desktop will start automatically. You'll see the Docker whale icon in your system tray.</p> </li> </ol>","tags":["Docker Desktop","MacOS","Windows"]},{"location":"container/install/#macos-installation","title":"macOS Installation","text":"","tags":["Docker Desktop","MacOS","Windows"]},{"location":"container/install/#prerequisites_1","title":"Prerequisites","text":"<p>Before installing Docker Desktop on macOS, ensure that your system meets the following requirements:</p> <ul> <li>macOS 10.13 or newer with macOS Sierra, High Sierra, or Mojave, and at least 4GB of RAM.</li> </ul>","tags":["Docker Desktop","MacOS","Windows"]},{"location":"container/install/#installation-steps_1","title":"Installation Steps","text":"<ol> <li> <p>Go to the Docker Desktop for Mac download page.</p> </li> <li> <p>Download the Docker Desktop for Mac installer.</p> </li> <li> <p>Double-click the installer to open it.</p> </li> <li> <p>Drag the Docker icon into your Applications folder to complete the installation.</p> </li> <li> <p>Launch Docker Desktop from your Applications folder.</p> </li> <li> <p>You may be prompted to provide your system password to allow Docker to make changes. Enter your password to proceed.</p> </li> <li> <p>Docker Desktop will start, and you'll see the Docker whale icon in your macOS menu bar.</p> </li> </ol>","tags":["Docker Desktop","MacOS","Windows"]},{"location":"container/install/#verifying-the-installation","title":"Verifying the Installation","text":"<p>To verify that Docker Desktop is installed and running correctly, open a terminal or command prompt and run the following command:</p> <p><pre><code>docker --version\n</code></pre> Output:</p> <pre><code>Docker version 24.0.6, build ed223bc\n</code></pre> <p>You should see the Docker version displayed, which confirms that Docker Desktop is installed and ready for use.</p> <p>Run the Container hello-world Container</p> <p>In your terminal or command prompt, type the following command and press Enter:</p> <p><pre><code>docker run hello-world\n</code></pre> Output:</p> <pre><code>Unable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\n719385e32844: Pull complete \nDigest: sha256:88ec0acaa3ec199d3b7eaf73588f4518c25f9d34f58ce9a0df68429c5af48e8d\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://hub.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/get-started/\n</code></pre> <p>That's it! You've successfully installed Docker Desktop on your Windows or macOS machine. You can now start creating and managing containers with ease.</p>","tags":["Docker Desktop","MacOS","Windows"]},{"location":"container/intro/","title":"Containers","text":"","tags":["Containers","Docker"]},{"location":"container/intro/#introduction","title":"Introduction","text":"<p>Containers have revolutionized the way we deploy and manage applications. In this document, we will explore the concept of containers, their advantages, and how they are transforming the world of software development and deployment.</p>","tags":["Containers","Docker"]},{"location":"container/intro/#what-are-containers","title":"What are Containers","text":"<p>Containers are lightweight, portable, and self-sufficient units that encapsulate an application and its dependencies. They provide a consistent environment for running applications across different platforms. Unlike traditional virtualization, containers share the host operating system's kernel, making them highly efficient and easy to manage.</p>","tags":["Containers","Docker"]},{"location":"container/intro/#characteristics-of-containers","title":"Characteristics of Containers","text":"<p>Containers exhibit the following key characteristics:</p> <ul> <li> <p>Isolation: Containers isolate applications from one another and from the host system, ensuring that each application runs in its own environment without interference.</p> </li> <li> <p>Portability: Container images are portable, allowing you to run the same application on any system that supports the container runtime.</p> </li> <li> <p>Efficiency: Containers are lightweight and efficient, as they share the host operating system's kernel and resources.</p> </li> <li> <p>Versioning: Container images can be versioned, making it easy to roll back to a previous version or upgrade to a newer one.</p> </li> <li> <p>Easy Deployment: Containers simplify the deployment process, enabling rapid application deployment and scaling.</p> </li> </ul>","tags":["Containers","Docker"]},{"location":"container/intro/#docker-vs-virtual-machines","title":"Docker vs Virtual Machines","text":"","tags":["Containers","Docker"]},{"location":"container/intro/#key-terminology-in-docker","title":"Key Terminology in Docker","text":"<p>Before we dive deeper into the world of Docker, it's essential to understand some key terminology that forms the foundation of container technology.</p>","tags":["Containers","Docker"]},{"location":"container/intro/#1-image","title":"1. Image","text":"<p>In Docker, an image is a lightweight, standalone, and executable package that contains everything needed to run an application, including the code, a runtime, libraries, environment variables, and configuration files. Images serve as the blueprint for containers. They are read-only and can be shared and distributed via container registries, making it easy to move applications between different environments.</p> <p>You can find a vast collection of pre-built Docker images on Docker Hub, which is a public container registry. Docker Hub allows you to search for, pull, and use images shared by the Docker community and other organizations. These images can save you time and effort by providing a starting point for your own Docker containers.</p> <p></p> <p>Docker images are often based on a base image and can be customized to include specific software and configurations.</p> <p>Examples:</p> <ul> <li> <p>Python :  Docker Official Python Image</p> <p>Python is an interpreted, interactive, object-oriented, open-source programming language.</p> </li> <li> <p>Postgres :  Docker Official Postgres Image</p> <p>The PostgreSQL object-relational database system provides reliability and data integrity.</p> </li> </ul> <p>In Docker, tags play a crucial role in versioning and managing Docker images. A tag is a label or identifier that is associated with a specific version of an image. Tags allow you to distinguish between different versions of an image and provide a way to reference and use a particular image.</p> <p></p>","tags":["Containers","Docker"]},{"location":"container/intro/#2-container","title":"2. Container","text":"<p>A container is an instance of a Docker image. It's a runnable process that includes not only the application code but also the necessary libraries and runtime environment. Containers are isolated from one another and from the host system, which ensures consistent and reliable application execution. Containers can be created, started, stopped, moved, and deleted, making them highly dynamic and versatile for deploying applications.</p>","tags":["Containers","Docker"]},{"location":"container/intro/#3-dockerfile","title":"3. Dockerfile","text":"<p>A Dockerfile is a text document that contains a set of instructions for building a Docker image. These instructions specify the base image, add files, install software, configure environment variables, and define the command to run when the container starts. Dockerfiles provide a way to automate the creation of container images, ensuring consistency and reproducibility. By following the instructions in a Dockerfile, you can build an image that precisely matches your application's requirements.</p> Dockerfile<pre><code># Use an official Python runtime as a parent image\nFROM python:3.8\n\n# Set the working directory in the container\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY . /app\n\n# Install any needed packages specified in requirements.txt\nRUN pip install --trusted-host pypi.python.org -r requirements.txt\n\n# Make port 80 available to the world outside this container\nEXPOSE 80\n\n# Define environment variable\nENV NAME World\n\n# Run app.py when the container launches\nCMD [\"python\", \"app.py\"]\n</code></pre>","tags":["Containers","Docker"]},{"location":"container/intro/#4-docker-compose","title":"4. Docker Compose","text":"<p>Docker Compose is a tool for defining and running multi-container Docker applications. It uses a YAML file to configure application services, networks, and volumes. Docker Compose simplifies the process of managing complex applications with multiple containers and their interactions. It's especially useful for orchestrating microservices-based architectures.</p> docker-compose.yml<pre><code>version: '3'\nservices:\n  web:\n    image: python:3.8\n    container_name: my-python-app\n    command: python app.py\n    ports:\n      - \"80:80\"\n</code></pre>","tags":["Containers","Docker"]},{"location":"container/intro/#conclusion","title":"Conclusion","text":"<p>Understanding these key terms is crucial for working effectively with Docker. Images provide the building blocks for containers, Dockerfiles define how images are created, and containers are the running instances of those images. Docker Compose further extends Docker's capabilities to manage complex applications and their components.</p>","tags":["Containers","Docker"]},{"location":"fastapi/Introduction/","title":"FastAPI","text":"","tags":["FastAPI"]},{"location":"fastapi/Introduction/#introduction","title":"Introduction","text":"<p>FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints. It's designed to be easy to use and provide high-performance, while also offering automatic, interactive documentation. This tutorial will guide you through the basics of FastAPI, so you can start building your web applications and APIs with ease.</p>","tags":["FastAPI"]},{"location":"fastapi/Introduction/#why-fastapi","title":"Why FastAPI?","text":"<p>FastAPI is gaining popularity rapidly, and here's why:</p> <ul> <li>Fast: It's one of the fastest web frameworks available for Python, thanks to its asynchronous support.</li> <li>Easy to Use: FastAPI leverages Python type hints to simplify API development and reduce the need for boilerplate code.</li> <li>Automatic Documentation: It generates interactive API documentation using OpenAPI and JSON Schema.</li> <li>Data Validation: It offers built-in data validation using Pydantic, helping to keep your data clean and reliable.</li> <li>Asynchronous Support: FastAPI supports asynchronous programming, which is particularly useful for handling I/O-bound operations efficiently.</li> <li>Scalable: It's designed for both small projects and large, high-traffic applications.</li> <li>Testable: It has excellent support for testing, making it easy to verify the correctness of your APIs.</li> </ul> <p>In this tutorial, you'll get started with FastAPI, create your first FastAPI application, understand routing, handle requests, and explore the fundamentals of building APIs with this powerful framework.</p> <p>Before we dive into the code, make sure you have Python 3.7 or higher installed and are ready to begin your journey with FastAPI.</p> <p>Let's get started!</p>","tags":["FastAPI"]},{"location":"fastapi/Introduction/#installation-and-setup","title":"Installation and Setup","text":"<p>Before we dive into building FastAPI applications, let's make sure you have FastAPI and its dependencies installed. We'll also create a virtual environment to keep your project dependencies isolated.</p>","tags":["FastAPI"]},{"location":"fastapi/Introduction/#step-1-prerequisites","title":"Step 1: Prerequisites","text":"<p>Before proceeding, ensure you have the following prerequisites in place:</p> <ul> <li>Python: Streamlit is a Python library, so you'll need Python installed. If you haven't already, you can download Python here. Recommended version 3.x+</li> </ul>","tags":["FastAPI"]},{"location":"fastapi/Introduction/#step-2-create-a-virtual-environment","title":"Step 2: Create a Virtual Environment","text":"<p>It's a good practice to create a virtual environment for your FastAPI project to manage dependencies cleanly. In this tutorial, we'll use pipenv to create and manage the virtual environment.</p> <pre><code>pipenv shell\n</code></pre> <p>You'll notice your terminal prompt changes, indicating that you are now inside the virtual environment. Any packages you install or scripts you run will be isolated within this environment.</p>","tags":["FastAPI"]},{"location":"fastapi/Introduction/#step-3-install-production-dependencies","title":"Step 3: Install Production Dependencies","text":"<p>FastAPI, Pandas, Gunicorn, and Uvicorn, along with their required dependencies, will be installed in your virtual environment. These dependencies are suitable for production use.</p> <pre><code>pipenv install fastapi pandas gunicorn uvicorn\n</code></pre>","tags":["FastAPI"]},{"location":"fastapi/Introduction/#step-4-install-development-dependencies","title":"Step 4: Install Development Dependencies","text":"<p>For development and testing, you can install additional dependencies. Run the following command to install development dependencies:</p> <p><pre><code>pipenv install pytest-cov httpx\n</code></pre> These dependencies include pytest-cov for testing coverage and httpx for making HTTP requests during development.</p>","tags":["FastAPI"]},{"location":"fastapi/Introduction/#step-5-create-a-new-python-file","title":"Step 5: Create a New Python File","text":"<p>Start by creating a new Python file in your preferred code editor or IDE. Name it, for example, <code>main.py</code>.</p>","tags":["FastAPI"]},{"location":"fastapi/Introduction/#step-6-import-fastapi","title":"Step 6: Import FastAPI","text":"<p>Inside your <code>main.py</code> file, import FastAPI by adding the following line at the top:</p> <pre><code>import uvicorn\nimport pandas as pd\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom fastapi.responses import JSONResponse\n</code></pre>","tags":["FastAPI"]},{"location":"fastapi/Introduction/#step-7-create-an-instance-of-fastapi","title":"Step 7: Create an Instance of FastAPI","text":"<p>Now, create an instance of the FastAPI class to represent your web application. Add the following line to your main.py file:</p> <pre><code>app = FastAPI()\n</code></pre>","tags":["FastAPI"]},{"location":"fastapi/Introduction/#step-8-define-a-route-and-endpoint","title":"Step 8: Define a Route and Endpoint","text":"<p>Let's define a route and an endpoint. In FastAPI, you use Python functions as endpoints. Create a \"Hello World\" endpoint that returns a JSON response:</p> <pre><code>@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello World\"}\n</code></pre> <ul> <li> <p><code>@app.get(\"/\")</code> is a decorator that specifies the HTTP method (GET) and the URL path (\"/\") that this endpoint will respond to.</p> </li> <li> <p>def read_root(): is a Python function that will be executed when someone accesses the root path (\"/\").</p> </li> <li> <p>return <code>{\"message\": \"Hello World\"}</code> returns a JSON response with a simple greeting.</p> </li> </ul>","tags":["FastAPI"]},{"location":"fastapi/Introduction/#step-9-run-your-application","title":"Step 9: Run Your Application","text":"<p>To run your FastAPI application, use Uvicorn. Open your terminal, navigate to the directory containing your main.py file, and run the following command:</p> <pre><code>uvicorn main:app --reload\n</code></pre> <ul> <li> <p><code>main:app</code> specifies the Python file (<code>main.py</code>) and the FastAPI app instance (<code>app</code>) to run</p> </li> <li> <p><code>--reload</code> enables auto-reloading, which is useful during development.</p> </li> </ul>","tags":["FastAPI"]},{"location":"fastapi/Introduction/#step-10-access-your-application","title":"Step 10: Access Your Application","text":"<p>Once the application is running, open your web browser or API client and access your FastAPI application. You should see the \"Hello World\" message displayed as a JSON response.</p> <p>Congratulations! You've successfully created your first FastAPI application, following each step. This simple example demonstrates the basics of routing and handling requests with FastAPI. You can build on this foundation to create more complex and feature-rich APIs.</p>","tags":["FastAPI"]},{"location":"fastapi/Introduction/#understanding-the-fastapi-application","title":"Understanding the FastAPI Application","text":"<p>In this section, we'll break down the structure of the FastAPI application code you've provided and explain the purpose of each component. This will give you a clearer understanding of how FastAPI works.</p> main.py<pre><code># Import necessary modules and packages\nimport uvicorn  # To run the FastAPI application\nimport pandas as pd  # For data handling\nfrom fastapi import FastAPI  # FastAPI framework\nfrom pydantic import BaseModel  # For defining request body models\nfrom fastapi.responses import JSONResponse  # For returning JSON responses\n\n# Create a FastAPI instance\napp = FastAPI()\n\n# Define a Pydantic model to validate user input\nclass UserInput(BaseModel):\n    state: str  # Define the input field 'state' as a string\n\n# Function to load data from a remote source\ndef load_df():\n    # Define column specifications to parse a fixed-width file\n    cols = [\n        (20, 51),    # Name\n        (72, 75),    # ST\n        (106, 116),  # Lat\n        (116, 127)   # Lon\n    ]\n\n    # Read data from the provided URL and apply column specifications\n    df = pd.read_fwf(\"https://www.ncei.noaa.gov/access/homr/file/nexrad-stations.txt\", colspecs=cols, skiprows=[1])\n\n    # Filter out rows where the 'ST' column is not NaN (not missing)\n    df = df[df['ST'].notna()]\n    return df\n\n# Define a route for a health check endpoint\n@app.get(\"/api/v1/healthcheck\")\nasync def say_hello() -&gt; dict:\n    return {\"message\": \"Ok\"}  # Return a JSON response indicating that the service is healthy\n\n# Define a default route for the root endpoint\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello World\"}  # Return a simple \"Hello World\" message as a JSON response\n\n# Define a route to fetch all data\n@app.get(\"/api/v1/fetch_all_data\")\ndef fetch_data() -&gt; JSONResponse:\n    # Load data using the load_df function\n    df = load_df()\n    return JSONResponse(content=df.to_dict(orient='records'))  # Return the data as a JSON response\n\n# Define a route to fetch data filtered by a specific state\n@app.post(\"/api/v1/fetch_by_state\")\ndef fetch_data(userinput: UserInput) -&gt; JSONResponse:\n    # Load data using the load_df function\n    df = load_df()\n\n    # Filter the data based on the user's specified state (converted to uppercase)\n    filtered_df = df[df['ST'] == userinput.state.upper()]\n\n    return JSONResponse(content=filtered_df.to_dict(orient='records'))  # Return the filtered data as a JSON response\n</code></pre>","tags":["FastAPI"]},{"location":"fastapi/Introduction/#additional-resources","title":"Additional Resources","text":"<p>In your journey to mastering FastAPI, you may find the following resources valuable for further learning and exploration:</p>","tags":["FastAPI"]},{"location":"fastapi/Introduction/#official-fastapi-documentation","title":"Official FastAPI Documentation","text":"<ul> <li>FastAPI Official Documentation: The official documentation is an invaluable resource for understanding every aspect of FastAPI, including detailed explanations, tutorials, and advanced features.</li> </ul>","tags":["FastAPI"]},{"location":"fastapi/Introduction/#fastapi-github-repository","title":"FastAPI GitHub Repository","text":"<ul> <li>FastAPI GitHub Repository: Explore the source code, report issues, and contribute to the development of FastAPI on GitHub.</li> </ul>","tags":["FastAPI"]},{"location":"fastapi/Introduction/#fastapi-user-guide","title":"FastAPI User Guide","text":"<ul> <li>FastAPI User Guide: This section of the documentation provides practical guidance on using FastAPI for various tasks and use cases.</li> </ul> <p>These resources offer a wealth of information, tutorials, and community support to help you enhance your FastAPI skills and build powerful web applications and APIs. Don't hesitate to explore them and continue your journey of learning and building with FastAPI.</p>","tags":["FastAPI"]},{"location":"fastapi/ci_testing/","title":"Continuous Integration (CI) Testing with GitHub Actions","text":"<p>Continuous Integration (CI) is a software development practice that involves automatically testing and building your code every time changes are pushed to a version control repository. GitHub Actions is a powerful CI/CD (Continuous Integration/Continuous Deployment) platform provided by GitHub, and it can be used to automate the testing of your FastAPI application. In this section, we'll explore how to set up CI testing for your FastAPI project using GitHub Actions.</p>","tags":["FastAPI","pytest","GithubAction"]},{"location":"fastapi/ci_testing/#prerequisites","title":"Prerequisites","text":"<p>Before setting up CI testing with GitHub Actions, ensure you have the following:</p> <ol> <li> <p>A GitHub repository that contains your FastAPI project.</p> </li> <li> <p>A working knowledge of GitHub and GitHub Actions.</p> </li> </ol>","tags":["FastAPI","pytest","GithubAction"]},{"location":"fastapi/ci_testing/#creating-a-github-actions-workflow","title":"Creating a GitHub Actions Workflow","text":"<p>GitHub Actions workflows are defined in <code>.yaml</code> files within your repository. These workflows specify the series of steps to run when specific events occur. To set up CI testing, follow these steps:</p>","tags":["FastAPI","pytest","GithubAction"]},{"location":"fastapi/ci_testing/#step-1-create-a-githubworkflows-directory","title":"Step 1: Create a <code>.github/workflows</code> Directory","text":"<p>In your GitHub repository, create a <code>.github/workflows</code> directory if it doesn't already exist. This is where you'll store your GitHub Actions workflow files.</p>","tags":["FastAPI","pytest","GithubAction"]},{"location":"fastapi/ci_testing/#step-2-create-a-workflow-yaml-file","title":"Step 2: Create a Workflow YAML File","text":"<p>Inside the <code>.github/workflows</code> directory, create a new YAML file, such as <code>fastapi-ci.yml</code>, and define your CI testing workflow.</p> <p>Here's a basic example of a CI testing workflow for a FastAPI project:</p> fastapi-ci.yml<pre><code>name: fastapi-ci  # Name your CI/CD workflow, e.g., \"fastapi-ci\"\n\non:\n  push:\n    paths:\n      - 'fastapi/**'  # Trigger this workflow when changes are pushed to the 'fastapi/' directory\n      # TODO: Change the path to match the directory you want to monitor for changes\n\njobs:\n  unit-tests:\n    runs-on: ubuntu-latest  # Use the latest version of the Ubuntu runner\n\n    steps:\n      - \n        name: Checkout  # Step to check out the code from the repository\n        uses: actions/checkout@v3\n\n      - \n        id: commit\n        uses: pr-mpt/actions-commit-hash@v2  # Step to retrieve the commit hash\n\n      - \n        name: Set up Python 3.12  # Step to set up Python 3.12 for testing\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.12'  # Specify the Python version you want to use\n          # TODO: Change to your desired Python version\n\n      - \n        name: Install pipenv  # Step to install the pipenv package manager\n        run: pip install pipenv\n\n      - \n        name: Run tests  # Step to run unit tests\n        working-directory: ./fastapi  # Change to the directory where your tests are located\n        run: |\n          pipenv install --dev  # Install project dependencies and dev dependencies using pipenv\n          pipenv run coverage run -m pytest -v .  # Run Pytest and measure test coverage\n          pipenv run coverage report -m  # Generate and display coverage report\n        # You can also run Pytest without coverage report using: pipenv run pytest -v .\n</code></pre> <p>This YAML file sets up a workflow named \"CI Testing\" that triggers on pushes to the main branch. It runs the tests using Python 3.x, installs dependencies, runs Pytest, and uploads test results.</p>","tags":["FastAPI","pytest","GithubAction"]},{"location":"fastapi/ci_testing/#step-3-commit-and-push-the-yaml-file","title":"Step 3: Commit and Push the YAML File","text":"<p>Commit the fastapi-ci.yml file to your repository and push it to GitHub. The workflow will automatically be activated on pushes to the main branch.</p>","tags":["FastAPI","pytest","GithubAction"]},{"location":"fastapi/ci_testing/#step-4-monitor-ci-testing","title":"Step 4: Monitor CI Testing","text":"<p>You can monitor the progress of your CI testing workflow by visiting the \"Actions\" tab in your GitHub repository. There, you'll find detailed logs and information about each run.</p>","tags":["FastAPI","pytest","GithubAction"]},{"location":"fastapi/ci_testing/#conclusion","title":"Conclusion","text":"<p>That's it! You've now set up CI testing for your FastAPI project using GitHub Actions. With CI testing in place, your code will be automatically tested every time changes are pushed to your repository, ensuring the reliability of your application.</p>","tags":["FastAPI","pytest","GithubAction"]},{"location":"fastapi/ci_testing/#further-reading","title":"Further Reading","text":"<ul> <li>GitHub Actions Official Documentation: The official documentation is a comprehensive resource for understanding and using GitHub Actions for CI/CD.</li> </ul>","tags":["FastAPI","pytest","GithubAction"]},{"location":"fastapi/unit_test/","title":"Unit Testing with Pytest","text":"<p>Unit testing is a crucial aspect of software development. In this section, you'll learn how to write unit tests for your FastAPI application using the Pytest testing framework. We'll use the <code>TestClient</code> provided by FastAPI to send HTTP requests to your application and verify the expected responses.</p>","tags":["FastAPI","pytest"]},{"location":"fastapi/unit_test/#prerequisites","title":"Prerequisites","text":"<p>Before getting started with unit testing, ensure that you have <code>Pytest</code> and <code>httpx</code> installed. You can install it using pipenv:</p>","tags":["FastAPI","pytest"]},{"location":"fastapi/unit_test/#instructions","title":"Instructions","text":"","tags":["FastAPI","pytest"]},{"location":"fastapi/unit_test/#step-1-setting-up-testclient","title":"Step 1: Setting up TestClient","text":"<p>To begin unit testing, set up the TestClient for your FastAPI application. The TestClient is a convenient tool for sending HTTP requests and receiving responses within your tests. Here's an example of how to set it up:</p> <pre><code># test_main.py\n\nfrom fastapi.testclient import TestClient\nfrom main import app\n\nclient = TestClient(app)\n</code></pre> <p>Make sure to import the TestClient and your FastAPI application (app) from your main application file.</p>","tags":["FastAPI","pytest"]},{"location":"fastapi/unit_test/#step-2-writing-unit-tests","title":"Step 2: Writing Unit Tests","text":"<p>Now, let's write some unit tests for your FastAPI application. We'll provide a few examples to get you started.</p>","tags":["FastAPI","pytest"]},{"location":"fastapi/unit_test/#test-the-health-check-endpoint","title":"Test the Health Check Endpoint","text":"<p>The following test checks the health check endpoint:</p> <pre><code>def test_healthcheck():\n    response = client.get(\n        url=\"/api/v1/healthcheck\"\n    )\n    assert response.status_code == 200\n    message = response.json()[\"message\"]\n    assert message == \"Ok\"\n</code></pre> <p>This test sends a GET request to the <code>/api/v1/healthcheck</code> endpoint and verifies that the response status code is <code>200</code> (indicating success) and that the response message is <code>\"Ok\"</code></p>","tags":["FastAPI","pytest"]},{"location":"fastapi/unit_test/#test-the-fetch_all_data-endpoint","title":"Test the fetch_all_data Endpoint","text":"<p>You can test an endpoint that fetches all data: <pre><code>def test_fetch_all_data():\n    response = client.get(\n        url=\"/api/v1/fetch_all_data\"\n    )\n    assert response.status_code == 200\n    assert len(response.json()) == 205\n</code></pre></p> <p>This test sends a GET request to the <code>/api/v1/fetch_all_data</code> endpoint and checks that the response status code is <code>200</code>. It also verifies that the response contains <code>205</code> items (modify this number based on your data).</p>","tags":["FastAPI","pytest"]},{"location":"fastapi/unit_test/#test-the-fetch_by_state-endpoint","title":"Test the fetch_by_state Endpoint","text":"<p>You can write tests for an endpoint that fetches data by state: <pre><code>def test_fetch_by_state_ma():\n    response = client.post(\n        url=\"/api/v1/fetch_by_state\",\n        json={\n            \"state\": \"ma\"\n        }\n    )\n    assert response.status_code == 200\n    assert len(response.json()) == 2\n</code></pre> This test sends a POST request to the <code>/api/v1/fetch_by_state</code> endpoint with a JSON payload. It checks that the response status code is <code>200</code> and that the response contains <code>2</code> items. You can write similar tests for different states.</p>","tags":["FastAPI","pytest"]},{"location":"fastapi/unit_test/#step-3-running-the-tests","title":"Step 3: Running the Tests","text":"<p>You can run your tests using the following command:</p> <pre><code>pytest test_main.py\n</code></pre> <p>Test Report with Code coverage report</p> stdout<pre><code>============================= test session starts ==============================\nplatform linux -- Python 3.12.0, pytest-7.4.2, pluggy-1.3.0 -- /home/runner/.local/share/virtualenvs/fastapi-ujMFs4rz/bin/python\ncachedir: .pytest_cache\nrootdir: /home/runner/work/DAMG7245-Fall2023/DAMG7245-Fall2023/fastapi\nplugins: anyio-3.7.1, cov-4.1.0\ncollecting ... collected 5 items\n\ntest_main.py::test_healthcheck PASSED                                    [ 20%]\ntest_main.py::test_fetch_all_data PASSED                                 [ 40%]\ntest_main.py::test_fetch_by_state_ma PASSED                              [ 60%]\ntest_main.py::test_fetch_by_state_MA PASSED                              [ 80%]\ntest_main.py::test_fetch_by_state_MAA PASSED                             [100%]\n\n=============================== warnings summary ===============================\n../../../../.local/share/virtualenvs/fastapi-ujMFs4rz/lib/python3.12/site-packages/dateutil/tz/tz.py:37\n  /home/runner/.local/share/virtualenvs/fastapi-ujMFs4rz/lib/python3.12/site-packages/dateutil/tz/tz.py:37: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n    EPOCH = datetime.datetime.utcfromtimestamp(0)\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n========================= 5 passed, 1 warning in 5.96s =========================\nName           Stmts   Miss  Cover   Missing\n--------------------------------------------\nmain.py           28      1    96%   29\ntest_main.py      24      0   100%\n--------------------------------------------\nTOTAL             52      1    98%\n</code></pre>","tags":["FastAPI","pytest"]},{"location":"fastapi/unit_test/#conclusion","title":"Conclusion","text":"<p>Writing unit tests for your FastAPI application is crucial for ensuring the correctness of your endpoints and data. These tests help identify issues early in the development process and provide confidence in the reliability of your API.</p>","tags":["FastAPI","pytest"]},{"location":"fastapi/unit_test/#further-reading","title":"Further Reading","text":"<ul> <li> <p>Pytest Official Documentation: The official documentation is an excellent resource for getting started with Pytest and mastering its features.</p> </li> <li> <p>Effective Pytest by Martin Thoma: This guide provides practical tips and best practices for writing effective Pytest tests.</p> </li> </ul>","tags":["FastAPI","pytest"]},{"location":"gcp/Introduction/","title":"Google Cloud","text":"","tags":["GCP"]},{"location":"gcp/Introduction/#introduction","title":"Introduction","text":"<p>Google Cloud, often referred to as GCP (Google Cloud Platform), is a cutting-edge cloud computing platform and suite of cloud services offered by Google. Designed to meet the diverse needs of individuals, businesses, and organizations, Google Cloud empowers users to harness the power of the cloud for a wide range of applications, from hosting websites to advanced machine learning and data analytics.</p> <p>With a global network of data centers, Google Cloud provides a robust infrastructure that is not only highly available but also highly secure. It offers a comprehensive set of tools, services, and solutions, enabling users to build, deploy, and manage applications and workloads efficiently and securely.</p> <p>Google Cloud's services span various domains, including infrastructure, data analytics, machine learning, and developer tools. From virtual machines and cloud storage to Big Data solutions, AI and ML services, and APIs, Google Cloud caters to the needs of both developers and enterprises, delivering the resources required to innovate and succeed in the digital era.</p>","tags":["GCP"]},{"location":"gcp/Introduction/#top-google-cloud-platform-services","title":"Top Google Cloud Platform Services","text":"<p>Google Cloud Platform (GCP) offers a wide range of services to cater to various cloud computing needs. Here are some of the top services and categories:</p> <ol> <li> <p>Compute Engine: Allows you to create and manage virtual machines (VMs) in the cloud, suitable for various workloads and applications.</p> </li> <li> <p>Google Kubernetes Engine (GKE): A managed Kubernetes service that simplifies container orchestration, making it easy to deploy, manage, and scale containerized applications.</p> </li> <li> <p>Google App Engine: A Platform-as-a-Service (PaaS) offering that enables developers to build and deploy applications without managing the underlying infrastructure.</p> </li> <li> <p>Cloud Functions: A serverless compute platform for running single-purpose, event-driven functions without provisioning or managing servers.</p> </li> <li> <p>Google Cloud Storage: Offers scalable and highly available object storage for storing and retrieving any amount of data, including multimedia and data backups.</p> </li> <li> <p>BigQuery: A fully-managed, serverless, and highly scalable data warehouse for running super-fast SQL queries using the processing power of Google's infrastructure.</p> </li> </ol> <p>Now, let's embark on a journey of exploration and discovery as we delve into the rich ecosystem of Google Cloud services and learn how to harness the cloud for your unique needs.</p>","tags":["GCP"]},{"location":"gcp/Introduction/#prerequisites","title":"Prerequisites","text":"<p>Before you can start provisioning a Compute Engine instance on GCP and connecting to VS Code, there are a few prerequisites you need to complete:</p>","tags":["GCP"]},{"location":"gcp/Introduction/#step-1-create-a-gmail-account-for-300-free-credit-on-gcp","title":"Step 1: Create a Gmail Account for $300 Free Credit on GCP","text":"<ul> <li>To begin your journey with GCP, you'll need a Google account. If you don't have one already, create a new Gmail account. This account will be used to access GCP services.</li> </ul> <p>Why do you need a Gmail account?</p> <p>A Gmail account is a prerequisite because Google offers $300 in free credits to new GCP users, giving you the opportunity to explore and experiment with various GCP services at no initial cost.</p> <p>How to create a Gmail account:</p> <p>If you don't have a Gmail account, you can create one by following these steps:</p> <ul> <li>Go to Gmail Sign-Up.</li> <li>Fill in the required information, including your first and last name, desired email address, and a strong password.</li> <li>Follow the on-screen instructions to complete the account creation process.</li> </ul>","tags":["GCP"]},{"location":"gcp/Introduction/#step-2-create-a-google-cloud-platform-account-and-set-up-billing","title":"Step 2: Create a Google Cloud Platform Account and Set Up Billing","text":"<ul> <li>After creating your Gmail account, the next step is to create a Google Cloud Platform (GCP) account. GCP provides a wide range of cloud computing services, and you'll use this account to access them.</li> </ul> <p>Why do you need a GCP account?</p> <p>A GCP account is essential for provisioning virtual machines and other cloud resources, including Compute Engine instances, which you will use in this guide.</p> <p>How to create a GCP account and set up billing:</p> <p>To create a GCP account and set up billing, follow these steps:</p> <ul> <li>Visit the Google Cloud Platform website.</li> <li>Click on the \"Get Started for Free\" or \"Console\" button to begin the registration process.</li> <li>Follow the on-screen instructions to provide the necessary information, such as your business name, contact information, and payment details.</li> </ul> <p>Important: As part of the account setup, you will be required to enter billing information. While you may incur charges based on your usage of GCP services, you can still take advantage of the $300 free credit for initial exploration and learning.</p> <p>Once you've completed these prerequisites, you're ready to move on to provisioning a Compute Engine instance on GCP and connecting to it using VS Code. Continue to the next sections for detailed instructions.</p>","tags":["GCP"]},{"location":"gcp/compute_engine/","title":"Provisioning a Compute Engine on Google Cloud Platform (GCP) and Connecting to VS Code","text":"","tags":["GCP","Compute Engine"]},{"location":"gcp/compute_engine/#infrastructure-setup","title":"Infrastructure Setup","text":"<p>Welcome to the guide on setting up your infrastructure on Google Cloud Platform (GCP) and connecting to Visual Studio Code (VS Code). This step-by-step guide will help you get started with GCP services.</p>","tags":["GCP","Compute Engine"]},{"location":"gcp/compute_engine/#step-1-log-into-the-gcp-console-and-create-a-project","title":"Step 1: Log into the GCP Console and Create a Project","text":"<ol> <li>Open your web browser and go to the Google Cloud Console.</li> <li>Sign in with your Google account credentials.</li> <li> <p>After signing in, you'll be taken to the GCP dashboard. Click on the project dropdown menu at the top of the screen and select \"New Project.\"</p> </li> <li> <p>In the \"New Project\" window, provide a name for your project. Let's name it <code>mlops-demo</code>.</p> </li> <li> <p>Click the \"Create\" button to create the project.</p> </li> </ol> <p></p>","tags":["GCP","Compute Engine"]},{"location":"gcp/compute_engine/#step-2-create-a-compute-engine-instance","title":"Step 2: Create a Compute Engine Instance","text":"<p>Now that you have your GCP project set up, let's create a virtual machine instance using Compute Engine.</p> <ol> <li> <p>In the GCP Console, ensure that you've selected the <code>mlops-demo</code> project in the project dropdown.</p> </li> <li> <p>In the left-hand navigation pane, scroll down and select \"Compute Engine\" under \"Compute.\"</p> </li> <li> <p>Click the \"Create\" button to create a new virtual machine instance.</p> </li> <li> <p>In the \"Create an instance\" form, fill in the following configuration details:</p> <ul> <li> <p>Name: Enter a name for your VM, e.g., <code>demo-vm</code>.</p> </li> <li> <p>Region: Select a region where you want to deploy the VM. For example, you can choose <code>us-east1</code>.</p> </li> <li> <p>Zone: Choose a zone within the selected region, for instance, <code>us-east1-b</code>.</p> </li> <li> <p>Series: Select a series, such as <code>N2</code>.</p> </li> <li> <p>Machine Type: Choose an appropriate machine type, like <code>n2-standard-2</code>.</p> </li> <li> <p>Boot Disk:</p> <ul> <li>Image: Select \"Ubuntu 22.04 LTS\" as the operating system image.</li> </ul> </li> <li> <p>Firewall: Check both options to allow incoming traffic:</p> <ul> <li>\"Allow HTTP traffic\"</li> <li>\"Allow HTTPS traffic\"</li> </ul> </li> </ul> </li> <li> <p>Review the configuration settings to ensure they match the above recommendations.</p> </li> <li> <p>Click the \"Create\" button to provision the VM. This process may take a few moments, so be patient.</p> </li> </ol> <p></p> <p></p>","tags":["GCP","Compute Engine"]},{"location":"gcp/compute_engine/#step-3-assign-a-static-public-ip-address","title":"Step 3: Assign a Static Public IP Address","text":"<p>To ensure that your Google Cloud Compute Engine instance has a consistent and publicly accessible IP address, you can reserve a static IP address. This step is especially useful when you need to connect to your instance using remote tools like Visual Studio Code. Here's how to assign a static public IP:</p> <ol> <li> <p>Access Network Details:</p> <ul> <li>In the Google Cloud Console, navigate to the \"Compute Engine\" section by clicking on the left-hand menu.</li> <li>Under the \"VM instances\" category, select the virtual machine (VM) instance for which you want to assign a static IP address. If you haven't created an instance yet, you can refer to the previous steps for instructions on how to create one.</li> </ul> </li> <li> <p>View IP Addresses:</p> <ul> <li>Within the selected VM instance's details page, click on the \"Edit\" button to make changes to the instance's configuration.</li> </ul> </li> <li> <p>Reserve a Static Address:</p> <ul> <li>In the \"Edit VM instance\" page, scroll down to the \"Network interfaces\" section.</li> <li>Locate the \"External IP\" dropdown menu, and click on it to reveal the available options.</li> <li>Select \"Create IP address.\"</li> </ul> </li> <li> <p>Configure the Static IP Address:</p> <ul> <li>In the \"Create an IP address\" dialog, you can specify a name for your static IP address.</li> <li>Leave the \"IP version\" set to IPv4 unless you have specific requirements for IPv6.</li> <li>You can also define the \"Region\" where you want to create the IP address. It's a good practice to choose a region that aligns with the region of your VM instance, although it's not a strict requirement.</li> </ul> </li> <li> <p>Create the Static IP Address:</p> <ul> <li>Click the \"Reserve\" button to create the static IP address. Once you click this, the IP address becomes reserved and associated with your project.</li> </ul> </li> </ol> <p></p> <p></p> <p>You have successfully assigned a static public IP address to your Compute Engine instance, making it easier to consistently access your virtual machine, whether for SSH connections or remote development using Visual Studio Code. This static IP address remains associated with your VM instance until you release it manually or make changes to the instance's configuration.</p>","tags":["GCP","Compute Engine"]},{"location":"gcp/compute_engine/#step-4-enabling-ssh-for-google-compute-engine","title":"Step 4: Enabling SSH for Google Compute Engine","text":"<p>In this step, we will generate an SSH key and configure it for use with Google Compute Engine (GCE). SSH (Secure Shell) keys allow you to securely access your virtual machine instances on Google Cloud Platform (GCP). Follow these instructions to enable SSH access.</p> <ol> <li> <p>Generate an SSH Key</p> <p>For Mac OS and Linux users, open your terminal. If you are using Windows, you may need to adapt the commands accordingly.</p> <p>Execute the following command to generate an SSH key pair. This command creates a new SSH key with the RSA encryption algorithm and saves it as <code>mlops-demo</code> in the <code>~/.ssh</code> directory. The <code>-C</code> flag is used to provide a comment to help identify the key.</p> <pre><code>ssh-keygen -t rsa -f ~/.ssh/mlops-demo -C mlops_zoomcamp\n</code></pre> </li> <li> <p>Display the Public Key</p> <p>To configure SSH access on Google Compute Engine, you will need to provide the public key. Display the content of the public key file using the cat command.</p> <p><pre><code>cat ~/.ssh/mlops-demo.pub\n</code></pre> This command will output the content of the mlops-demo.pub file, which is the public key you will use to access your GCE instance.</p> </li> <li> <p>Configure SSH Key on GCP</p> <p>To configure your SSH key on the Google Cloud Platform (GCP) console, follow these steps:</p> <ul> <li> <p>Log in to your GCP Console: Open a web browser, go to Google Cloud Console, and log in with your GCP account.</p> </li> <li> <p>Navigate to Compute Engine: In the GCP Console, click on \"Compute Engine\" in the main menu on the left-hand side. This will take you to the Compute Engine dashboard.</p> </li> <li> <p>Locate Your Instance: In the Compute Engine dashboard, locate and click on the virtual machine instance to which you want to add the SSH key.</p> </li> <li> <p>Edit Instance Settings: On the instance details page, click on the \"Edit\" button. This will allow you to modify the settings for the selected instance.</p> </li> <li> <p>Access SSH Keys Section: Scroll down the instance configuration page until you find the \"SSH Keys\" section. This is where you can manage your SSH keys.</p> </li> <li> <p>Add SSH Key: In the \"SSH Keys\" section, click on the \"Add item\" button. This will open a form for adding a new SSH key.</p> </li> <li> <p>Enter the Public Key: In the \"Key\" field of the form, paste the content of the <code>mlops-demo.pub</code> file that you displayed earlier using the <code>cat</code> command. This is the public key that you generated in the previous step.</p> </li> <li> <p>Save Configuration: After pasting your SSH key, click on the \"Save\" button to save your SSH key configuration.</p> </li> </ul> </li> </ol> <p></p> <p>Now, you have successfully configured your SSH key on Google Compute Engine. You can use this SSH key to securely connect to your GCE instance using tools like VS Code.</p>","tags":["GCP","Compute Engine"]},{"location":"gcp/compute_engine/#step-5-ssh-into-your-compute-engine-instance","title":"Step 5: SSH into Your Compute Engine Instance","text":"<p>To continue with your Google Compute Engine journey, it's essential to establish an SSH connection to your instance. This connection allows you to access your instance's terminal for various tasks.</p> <ol> <li> <p>Replace Your Instance's Public IP Address</p> <p>Start by substituting <code>35.196.19.116</code> in the following command with the actual public IP address of your Compute Engine instance. Additionally, you'll need your private SSH key file (referred to as <code>mlops-demo</code> in this example) for authentication.</p> <pre><code>ssh -i ~/.ssh/mlops-demo mlops_zoomcamp@35.196.19.116\n</code></pre> <p>This command initiates an SSH connection to your instance using your private key and the public IP address of the instance.</p> </li> <li> <p>Add Host to Hostname File for Easy Login</p> <p>To simplify the process of logging into your Compute Engine instance, you can add an entry to your SSH configuration file. This makes it easier to connect in the future without needing to remember the IP address and key file.</p> <p>First, open the SSH configuration file for editing:</p> <pre><code>nano ~/.ssh/config\n</code></pre> <p>Add Configuration Inside the configuration file, add the following configuration block. Be sure to replace the placeholders with your specific values</p> <pre><code>Host gcp-mlops_demo          # Your chosen host name\n    HostName 35.196.19.116  # VM Public IP (your instance's IP)\n    User mlops_zoomcamp     # VM user (your username)\n    IdentityFile ~/.ssh/mlops-demo  # Private SSH key file (your key file)\n    StrictHostKeyChecking no\n</code></pre> <ul> <li>Host: Replace <code>gcp-mlops_demo</code> with a name of your choice for easy reference.</li> <li>HostName: Replace <code>35.196.19.116</code> with your instance's public IP address.</li> <li>User: Replace <code>mlops_zoomcamp</code> with your username on the Compute Engine instance.</li> <li>IdentityFile: Replace <code>~/.ssh/mlops-demo</code> with the path to your private SSH key file.</li> <li>StrictHostKeyChecking: Setting it to <code>no</code> skips strict host key checking. Only do this if you are sure of your instance's authenticity.</li> </ul> <p>Using the Configured Host Once you've added the configuration to your SSH configuration file, you can connect to your instance using the chosen host name. In this example, we used gcp-mlops_demo. Replace it with your chosen host name if different:</p> <pre><code>ssh gcp-mlops_demo\n</code></pre> <p>This command establishes an SSH connection to your Compute Engine instance using the configuration you set up earlier.</p> </li> </ol>","tags":["GCP","Compute Engine"]},{"location":"gcp/compute_engine/#step-6-connecting-to-vs-code","title":"Step 6: Connecting to VS Code","text":"<p>To work with your Google Cloud Platform (GCP) Compute Engine instance using Visual Studio Code (VS Code), follow these steps:</p> <ol> <li> <p>Install the Remote SSH extension: Before you can connect to your Compute Engine, you need to install the \"Remote SSH\" extension in VS Code. Here's how you can do it:</p> <ul> <li>Open VS Code.</li> <li>Go to the Extensions view by clicking on the Extensions icon in the Activity Bar on the side.</li> <li>Search for \"Remote - SSH\" in the search bar.</li> <li>Click \"Install\" next to the extension named \"Remote - SSH.\"</li> <li>Click on the installed extension</li> <li>From the list of remote SSH host open <code>gcp-mlops_demo</code></li> </ul> </li> </ol> <p>Once you are connected, you can access your Compute Engine resources, edit code, run applications, and perform various tasks directly from Visual Studio Code, all while being securely connected to your remote instance.</p>","tags":["GCP","Compute Engine"]},{"location":"gcp/terraform/","title":"Provisioning a Compute Engine on Google Cloud Platform (GCP) using Terraform","text":"","tags":["GCP","Compute Engine","Terraform"]},{"location":"gcp/terraform/#introduction","title":"Introduction","text":"<p>Welcome to the comprehensive guide for setting up infrastructure on Google Cloud Platform (GCP) and establishing a connection to Visual Studio Code (VS Code)  using Terraform. </p>","tags":["GCP","Compute Engine","Terraform"]},{"location":"gcp/terraform/#prerequisite","title":"Prerequisite","text":"<ol> <li> <p>Install Terraform</p> <p>Install Terraform on your local machine. You can download the appropriate Terraform package for your operating system from the official Terraform website and follow the installation instructions for your platform. Ensure that Terraform is added to your system's PATH to enable running Terraform commands from the terminal or command prompt.</p> <p>To verify the installation, open a terminal or command prompt and type  <pre><code>terraform -v\n</code></pre> This command should display the installed Terraform version.</p> </li> <li> <p>Create Service Account</p> <ol> <li>Access your GCP Console using your credentials.</li> <li>Navigate to the IAM &amp; Admin section in the GCP Console.</li> <li>Select Service Accounts and click on Create Service Account</li> <li>Fill in the necessary details for the service account, such as name and description.</li> <li>Assign the role <code>Compute Admin</code> to the service account. This role grants the necessary permissions to manage Compute Engine resources.</li> <li>After creating the service account, generate and download the key for this service account as a JSON file.</li> <li>This key file contains the authentication credentials required by Terraform to manage resources on GCP.</li> <li>Save Key Contents to tfkey.json:</li> <li>Once the JSON key file is downloaded, open the file and copy its contents.</li> <li>Create a file named <code>tfkey.json</code> in the directory where you'll be working with your Terraform code.</li> <li>Paste the contents of the downloaded JSON key into the <code>tfkey.json</code> file.</li> </ol> Warning <p>Ensure the key file (<code>tfkey.json</code>) remains secure and isn't shared publicly, as it contains sensitive authentication information allowing access to your GCP resources.</p> </li> <li> <p>SSH Key Generation</p> <ol> <li>Use the following command to generate an SSH key pair:     <pre><code>ssh-keygen -t rsa -f ce -C ubuntu -b 2048\n</code></pre><ul> <li><code>-t rsa</code>: Specifies the type of key to create (RSA).</li> <li><code>-f ce</code>: Sets the file name of the generated key to 'ce' (You can change 'ce' to a name of your choice).</li> <li><code>-C ubuntu</code>: Provides a comment, typically the username or a descriptive tag.</li> <li><code>-b 2048</code>: Specifies the number of bits in the key (2048-bit key in this case).</li> </ul> </li> <li>Upon execution, you might be prompted to enter a passphrase to secure your private key. Press Enter for an empty passphrase if you prefer not to use one.</li> <li>The command will generate two files: <code>ce</code> (private key) and <code>ce.pub</code> (public key) in your current directory.</li> <li>The corresponding public key (<code>ce.pub</code>) can be shared with your Compute Engine instance or other services for authentication.</li> </ol> Warning <p>Ensure the private key (<code>ce</code>) remains secure and isn't shared publicly. This private key will be used to authenticate SSH connections to your Compute Engine instance.</p> </li> </ol>","tags":["GCP","Compute Engine","Terraform"]},{"location":"gcp/terraform/#tutorials","title":"Tutorials","text":"<ol> <li> <p>Update the .tfvars (Terraform variables) file.     terraform.tfvars<pre><code>project_id = \"emerald-road-403117\"\nregion = \"us-east1\"\nzone = \"us-east1-b\"\nname = \"class-demo-redis\"\nssh_key_filename = \"ce-tf\"\nmachine_type = \"e2-standard-4\"\nstorage = 100\n</code></pre></p> <ol> <li><code>project_id</code>: Replace the value with your specific GCP Project ID.</li> <li><code>region</code>: Set the desired region for deploying resources in GCP.</li> <li><code>zone</code>: Choose the appropriate zone within the specified region.</li> <li><code>name</code>: Define a unique name for the Compute Engine instance or resource.</li> <li><code>ssh_key_filename</code>: Enter the name of the SSH key file generated previously.</li> <li><code>machine_type</code>: Select the desired machine type for the Compute Engine instance.</li> <li><code>storage</code>: Specify the amount of storage required for the instance (in GB).</li> </ol> </li> <li> <p>Update the main.tf file to open the firewall ports</p> main.tf<pre><code>allow {\n    protocol = \"tcp\"\n    ports    = [\"80\", \"8080\", \"50001\", \"50002\", \"50003\", \"50004\"] # TODO: Change ports as required\n}\n</code></pre> </li> <li> <p>Initialize the Terraform Configuration</p> <p>Execute the following command in your terminal or command prompt to initialize your Terraform configuration:</p> <p><pre><code>terraform init\n</code></pre> This command initializes the working directory and prepares the configuration, ensuring all necessary plugins and modules are downloaded.</p> </li> <li> <p>Generate and Review an Execution Plan:</p> <p>To view the changes Terraform will apply before actually deploying the infrastructure, use the following command:</p> <p><pre><code>terraform plan\n</code></pre> This command generates an execution plan by examining the current state of your infrastructure and comparing it to the desired state defined in your configuration files (*.tf). Review the proposed changes in the plan output.</p> <p>Sample output: <pre><code>Plan: 3 to add, 0 to change, 0 to destroy.\n\nChanges to Outputs:\n+ ExternalIP = (known after apply)\n</code></pre></p> </li> <li> <p>Apply Changes to Deploy Infrastructure     Execute the following command to apply the changes defined in your Terraform configuration and deploy the infrastructure:</p> <p><pre><code>terraform apply\n</code></pre> The apply command implements the changes defined in your configuration files, creating or modifying the infrastructure according to the execution plan. It prompts for confirmation before making any changes.</p> <p>Sample output: <pre><code>Apply complete! Resources: 3 added, 0 changed, 0 destroyed.\n\nOutputs:\nExternalIP = \"34.139.43.201\"\n</code></pre></p> </li> <li> <p>SSH Connection using the Generated Key     Use the following command in your terminal to connect to your Compute Engine instance via SSH:</p> <p><pre><code>ssh -i ce ubuntu@34.139.43.201\n</code></pre> 1. <code>-i ce</code>: Specifies the identity file (private key) for authentication. Replace ce with the actual name of your SSH private key file. 2. <code>ubuntu@34.139.43.201</code>: Replace this with the actual username and IP address (or hostname) of your Compute Engine instance.</p> </li> <li> <p>Deploying Services and Docker Containers to Compute Engine</p> <p>To run your application services on your Compute Engine instance, follow these steps:</p> <ol> <li>Clone Repository and Prepare Environment</li> <li>Build Docker Image</li> <li>Run Docker Compose</li> </ol> <p>This will facilitate running your application services within containers on the remote server.</p> </li> <li> <p>Destroy Resources </p> Warning <p>Proceed once you no longer require the resources</p> <p>If you intend to remove all resources defined in your configuration, you can execute the following command:</p> <p><pre><code>terraform destroy\n</code></pre> The destroy command removes all resources defined in your Terraform configuration. It is irreversible and should be used judiciously, as it will delete the provisioned infrastructure. Sample output: <pre><code>Destroy complete! Resources: 3 destroyed.\n</code></pre></p> </li> </ol> <p>Continued exploration and further customization of the deployed resources can enhance and optimize the infrastructure, catering to specific project requirements and evolving needs.</p> <p>Happy provisioning and managing your infrastructure on GCP with Terraform!</p>","tags":["GCP","Compute Engine","Terraform"]},{"location":"gx/Introduction/","title":"Great Expectations","text":"","tags":["GX"]},{"location":"gx/Introduction/#introduction","title":"Introduction","text":"<p>Great Expectations is an open-source Python library that plays a crucial role in the realm of data validation and testing for data pipelines and data quality assurance. In a data-driven world, the reliability and trustworthiness of data are paramount, and Great Expectations helps data professionals, data engineers, and data scientists ensure that their data meets expectations and maintains quality.</p>","tags":["GX"]},{"location":"gx/Introduction/#key-concepts","title":"Key Concepts","text":"<ul> <li> <p>Data Validation: Great Expectations provides a framework for setting, documenting, and validating data expectations. It allows you to define the rules and criteria your data must adhere to, ensuring that it remains consistent and reliable.</p> </li> <li> <p>Data Quality: With Great Expectations, you can monitor and improve the quality of your data. By setting and continuously validating expectations, you can catch issues early, preventing bad data from propagating through your systems.</p> </li> <li> <p>Automated Testing: Great Expectations can be integrated into automated testing pipelines, providing confidence in the data's integrity as it moves through different stages of your data architecture.</p> </li> </ul> <p>In this tutorial, we will explore the core concepts and practical usage of Great Expectations, including setting expectations, validating data, and advanced features like data profiling and automated testing. Let's get started!</p>","tags":["GX"]},{"location":"gx/Introduction/#prerequisites","title":"Prerequisites","text":"<p>Before we dive into Great Expectations, make sure you have the following prerequisites in place:</p> <ul> <li> <p>Python: You should have Python installed on your system. If you don't have it installed, you can download it from the official Python website.</p> </li> <li> <p>Python Environment: It's a good practice to create a virtual environment for your Great Expectations project. You can use Python's built-in <code>venv</code> module or a third-party tool like <code>conda</code> or <code>virtualenv</code> to create an isolated environment.</p> </li> <li> <p>Great Expectations Installation: You'll need to install Great Expectations to use it. If you haven't already, you can install it using <code>pip</code> by running:</p> <pre><code>pip install great-expectations\n</code></pre> </li> <li> <p>Data Source: To follow along with the tutorial effectively, you should have access to a dataset or data source that you want to validate and test using Great Expectations.</p> </li> </ul> <p>Now that you've checked off these prerequisites, you're ready to start setting up and working with Great Expectations.</p>","tags":["GX"]},{"location":"gx/Introduction/#data-sources-for-great-expectations-gx","title":"Data Sources for Great Expectations (gx)","text":"<p>In this tutorial, we will explore three different data sources for working with Great Expectations (gx), each offering unique ways to validate and test your data:</p>","tags":["GX"]},{"location":"gx/Introduction/#1-local-file-system-csv-files","title":"1. Local File System (CSV Files)","text":"<ul> <li>The local file system serves as a common and convenient data source for many data validation tasks.</li> <li>You can validate data stored in CSV files, making it suitable for scenarios where your data is stored locally.</li> </ul>","tags":["GX"]},{"location":"gx/Introduction/#2-postgresql-database","title":"2. PostgreSQL Database","text":"<ul> <li>PostgreSQL is a powerful relational database system that allows you to manage, query, and analyze your data.</li> <li>We will demonstrate how to connect Great Expectations to a PostgreSQL database to set expectations and validate data stored in database tables.</li> </ul>","tags":["GX"]},{"location":"gx/Introduction/#3-aws-s3-object-storage","title":"3. AWS S3 Object Storage","text":"<ul> <li>Amazon S3 (Simple Storage Service) is a cloud-based object storage service provided by Amazon Web Services.</li> <li>You can use Great Expectations to connect to an AWS S3 bucket to validate data stored in cloud-based object storage.</li> </ul> <p>Each of these data sources has its own unique set of requirements and considerations, and we'll walk you through how to work with them using Great Expectations.</p>","tags":["GX"]},{"location":"gx/cloud_object/","title":"Cloud Object","text":"","tags":["GX","AWS","S3"]},{"location":"gx/cloud_object/#aws-s3-object-storage-csv-files","title":"AWS S3 Object Storage (CSV Files)","text":"<p>In this section, we will explore using Great Expectations to validate data stored in AWS S3, a popular object storage service provided by Amazon Web Services.</p> <p>In our accompanying Jupyter Notebook, you'll find detailed code examples and step-by-step instructions for each of these tasks. Let's start using Great Expectations to validate data stored in AWS S3 object storage!</p> <p>Notebook</p> <p>Refer this notebook notebooks/gx/gx_sql_db.ipynb</p>","tags":["GX","AWS","S3"]},{"location":"gx/cloud_object/#notebook-overview","title":"Notebook Overview","text":"","tags":["GX","AWS","S3"]},{"location":"gx/cloud_object/#initializing-the-great-expectations-directory","title":"Initializing the Great Expectations Directory","text":"<p>To begin, you'll need to initialize a Great Expectations directory for your project. This directory will store your data validation configurations, expectations, and checkpoints. You can use the \"init\" command to set up the directory structure. Detailed code for this initialization can be found in our accompanying Jupyter Notebook.</p>","tags":["GX","AWS","S3"]},{"location":"gx/cloud_object/#creating-a-data-source-for-aws-s3","title":"Creating a Data Source for AWS S3","text":"<p>Great Expectations allows you to create data sources that connect to your data stored in different locations. In this case, we will create a data source connecting to AWS S3. You will need to specify the name of the data source, the S3 bucket name where your data is stored, and other connection details. We will provide code examples in the notebook for creating this S3 data source, which will be named \"S3_NYC_Yellow_Taxi2.\"</p>","tags":["GX","AWS","S3"]},{"location":"gx/cloud_object/#creating-an-expectations-suite","title":"Creating an Expectations Suite","text":"<p>With the S3 data source set up, you can proceed to define expectations for your data. Expectations help you specify what you anticipate in your data, such as data types, value constraints, and other properties. The Jupyter Notebook contains code examples to guide you through creating an expectations suite for your S3-stored data.</p>","tags":["GX","AWS","S3"]},{"location":"gx/cloud_object/#creating-a-checkpoint","title":"Creating a Checkpoint","text":"<p>Checkpoints are crucial for monitoring your data quality over time. They capture the state of your data at specific points and help you track its quality. We will demonstrate how to create a checkpoint to monitor your S3 data against your defined expectations using Great Expectations.</p>","tags":["GX","AWS","S3"]},{"location":"gx/cloud_object/#viewing-the-data-report","title":"Viewing the Data Report","text":"<p>Great Expectations generates Data Docs that provide comprehensive reports on your data. These reports include data profiling, data validation results, and other valuable information. You can use Data Docs to gain insights into your data's quality and potential issues. We will show you how to view these reports in the notebook.</p>","tags":["GX","AWS","S3"]},{"location":"gx/local-fs/","title":"Local File System (CSV Files)","text":"<p>In this section, we will focus on using Great Expectations with a local file system, specifically working with CSV files. This is a common use case when you have data stored locally and want to validate it using Great Expectations.</p> <p>In our accompanying Jupyter Notebook, you will find detailed code examples and step-by-step instructions for each of these tasks. Let's get started with using Great Expectations to validate your local CSV data!</p> <p>Notebook</p> <p>Refer this notebook notebooks/gx/gx_local_csv.ipynb</p>","tags":["GX"]},{"location":"gx/local-fs/#notebook-overview","title":"Notebook Overview","text":"","tags":["GX"]},{"location":"gx/local-fs/#creating-a-data-source","title":"Creating a Data Source","text":"<p>To get started, you'll need to create a data source that points to the location of your CSV files. This step involves defining the structure and properties of your data source, such as the file path and column names.</p>","tags":["GX"]},{"location":"gx/local-fs/#setting-expectations","title":"Setting Expectations","text":"<p>Once you have defined your data source, the next step is to set expectations for your data. Expectations define the rules and criteria that your data must adhere to. You can specify expectations on data types, value ranges, uniqueness, and more. Our notebook provides code examples for setting expectations based on your specific data requirements.</p>","tags":["GX"]},{"location":"gx/local-fs/#creating-a-checkpoint","title":"Creating a Checkpoint","text":"<p>Checkpoints in Great Expectations allow you to capture the state of your data at a particular point in time. You can think of checkpoints as snapshots of your data that help you track its quality and integrity over time. We will guide you through the process of creating a checkpoint to monitor your data against your defined expectations.</p>","tags":["GX"]},{"location":"gx/local-fs/#viewing-the-data-report","title":"Viewing the Data Report","text":"<p>Great Expectations generates Data Docs that provide comprehensive reports on your data. These reports include data profiling, data validation results, and other relevant information. We will demonstrate how to use Data Docs to view detailed reports on your data, making it easier to understand its quality and any issues that may need attention.</p>","tags":["GX"]},{"location":"gx/sql-data/","title":"SQL DB","text":"","tags":["GX","PostgreSQL"]},{"location":"gx/sql-data/#postgresql-data-source","title":"PostgreSQL Data Source","text":"<p>In this section, we will explore using Great Expectations with a PostgreSQL data source. We'll be running PostgreSQL in a Docker container locally for demonstration purposes.</p> <p>In our accompanying Jupyter Notebook, you'll find detailed code examples and step-by-step instructions for each of these tasks. Let's start using Great Expectations to validate data in your locally running PostgreSQL database!</p> <p>Notebook</p> <p>Refer this notebook notebooks/gx/gx_sql_db.ipynb</p>","tags":["GX","PostgreSQL"]},{"location":"gx/sql-data/#notebook-overview","title":"Notebook Overview","text":"","tags":["GX","PostgreSQL"]},{"location":"gx/sql-data/#initializing-the-great-expectations-directory","title":"Initializing the Great Expectations Directory","text":"<p>Before diving into data validation, you'll need to initialize a Great Expectations directory for your project. This directory will store your data validation configurations, expectations, and checkpoints. You can use the \"init\" command to set up the directory structure. The detailed code for this can be found in our accompanying Jupyter Notebook.</p>","tags":["GX","PostgreSQL"]},{"location":"gx/sql-data/#inserting-data-into-the-database-table","title":"Inserting Data into the Database Table","text":"<p>To work with data in the PostgreSQL database, you'll need some sample data. We will insert data into a database table from an external source, in this case, the NOAA weather stations dataset. The data will be fetched from this URL. You will find the code for this data insertion in our notebook.</p>","tags":["GX","PostgreSQL"]},{"location":"gx/sql-data/#creating-a-data-source","title":"Creating a Data Source","text":"<p>After initializing the Great Expectations directory and loading data into your PostgreSQL database, the next step is to create a data source. This involves specifying the database connection details and the table you want to validate. The Jupyter Notebook contains code examples for creating a data source.</p>","tags":["GX","PostgreSQL"]},{"location":"gx/sql-data/#creating-an-expectations-suite","title":"Creating an Expectations Suite","text":"<p>Once you have your data source set up, you can start defining expectations for your data. Expectations help you specify what you anticipate in your data, such as data types, value constraints, and other properties. We will provide code examples in the notebook to guide you through creating an expectations suite.</p>","tags":["GX","PostgreSQL"]},{"location":"gx/sql-data/#creating-a-checkpoint","title":"Creating a Checkpoint","text":"<p>Checkpoints are essential for monitoring your data over time. They capture the state of your data at specific moments and help you track its quality. We will demonstrate how to create a checkpoint to monitor your PostgreSQL data against your expectations using Great Expectations.</p>","tags":["GX","PostgreSQL"]},{"location":"gx/sql-data/#viewing-the-data-report","title":"Viewing the Data Report","text":"<p>Great Expectations generates Data Docs that offer detailed reports on your data. These reports include data profiling, data validation results, and other valuable information. You can use Data Docs to get insights into your data's quality and any potential issues. We'll show you how to view these reports in the notebook.</p>","tags":["GX","PostgreSQL"]},{"location":"streamlit/Introduction/","title":"Streamlit","text":"","tags":["Streamlit"]},{"location":"streamlit/Introduction/#introduction","title":"Introduction","text":"<p>Streamlit is an open-source Python library that makes it incredibly easy to create web applications for data science and machine learning projects. It's designed for data professionals, engineers, and developers who want to quickly turn data scripts into shareable web apps.</p>","tags":["Streamlit"]},{"location":"streamlit/Introduction/#why-streamlit","title":"Why Streamlit?","text":"<ul> <li> <p>Simplicity: Streamlit is known for its simplicity. With just a few lines of Python code, you can create interactive web applications.</p> </li> <li> <p>Rapid Development: It allows for rapid development and prototyping, enabling you to go from data analysis to a functional web app in a matter of minutes.</p> </li> <li> <p>Integration: Streamlit integrates seamlessly with popular data science libraries like Pandas, Matplotlib, and Plotly, making it an ideal choice for data visualization.</p> </li> <li> <p>Customization: While Streamlit is easy for beginners, it also provides options for customization and advanced features for more experienced developers.</p> </li> </ul> <p>In this section, we will introduce you to the world of Streamlit and show you how to get started with creating your own data-driven web applications.</p>","tags":["Streamlit"]},{"location":"streamlit/Introduction/#installation-and-setup","title":"Installation and Setup","text":"<p>Before you can start building web applications with Streamlit, it's a good practice to create a virtual environment for your project. This isolates your project's dependencies from the system-wide Python installation. Follow these steps to create a virtual environment and install Streamlit:</p>","tags":["Streamlit"]},{"location":"streamlit/Introduction/#step-1-prerequisites","title":"Step 1: Prerequisites","text":"<p>Before proceeding, ensure you have the following prerequisites in place:</p> <ul> <li>Python: Streamlit is a Python library, so you'll need Python installed. If you haven't already, you can download Python here. Recommended version 3.x+</li> </ul>","tags":["Streamlit"]},{"location":"streamlit/Introduction/#step-2-create-a-virtual-environment","title":"Step 2: Create a Virtual Environment","text":"<ol> <li>Open your terminal or command prompt.</li> <li>Navigate to the directory where you want to create your project.</li> <li>Create a virtual environment by running the following command:</li> </ol> <pre><code>python -m venv myenv\n</code></pre>","tags":["Streamlit"]},{"location":"streamlit/Introduction/#step-3-activate-the-virtual-environment","title":"Step 3: Activate the Virtual Environment","text":"<pre><code>source myenv/bin/activate\n</code></pre>","tags":["Streamlit"]},{"location":"streamlit/Introduction/#step-4-install-streamlit","title":"Step 4: Install Streamlit","text":"<p>Now that you have activated your virtual environment, you can install Streamlit. Run the following command:</p> <pre><code>pip install streamlit\n</code></pre> <p>This will download and install Streamlit and its dependencies within your virtual environment.</p>","tags":["Streamlit"]},{"location":"streamlit/Introduction/#step-6-create-your-first-streamlit-app","title":"Step 6: Create Your First Streamlit App","text":"<p>Now that Streamlit is installed in your virtual environment, you can create your first Streamlit app. Create a Python script (e.g., my_app.py) and start building your app using Streamlit's simple API.</p> <p>Here's an example of a basic Streamlit app:</p> main.py<pre><code>import streamlit as st\n\nst.title('My First Streamlit App')\nst.write('Welcome to your first Streamlit app!')\n</code></pre> <p>Save the script, open your terminal, navigate to the directory where the script is located, and run:</p> <pre><code>streamlit run my_app.py\n</code></pre> <p>This will launch your Streamlit app in a new browser window.</p> <p></p> <p>You're now ready to start building and deploying Streamlit applications for your data science and machine learning projects.</p> <p>Note: Streamlit is continuously evolving, so make sure to check the official documentation for the most up-to-date installation.</p>","tags":["Streamlit"]},{"location":"streamlit/Introduction/#step-7-export-requirements-to-requirementstxt","title":"Step 7: Export Requirements to requirements.txt","text":"<p>Before you commit your code to github, it's a good practice to export the project's dependencies to a requirements.txt file. This makes it easier to recreate the same environment on another machine or share it with others. To do this, run the following command:</p> <pre><code>pip freeze &gt; requirements.txt\n</code></pre> <p>This command will capture the list of installed packages in your virtual environment and save them to a file named requirements.txt in your project directory.</p>","tags":["Streamlit"]},{"location":"streamlit/Introduction/#step-8-best-practices-and-gitignore","title":"Step 8: Best Practices and Gitignore","text":"Warning <p>Never commit sensitive data or your Python virtual environment to your version control system.</p> <p>When working on your Streamlit projects, it's important to follow best practices and take precautions to secure sensitive data. Here are some important considerations:</p> <ul> <li> <p>Use a <code>.gitignore</code> file: To avoid accidentally committing sensitive information to your version control system, such as your Python virtual environment or files containing sensitive data, make sure to create and maintain a <code>.gitignore</code> file in your project directory. This file lists the files and directories that should be excluded from version control.</p> .gitignore<pre><code>myenv/\n.env\n</code></pre> <p>In the example above, we've added <code>myenv/</code> to exclude the virtual environment and <code>.env</code> to exclude any files containing sensitive data.</p> </li> <li> <p>Never commit your Python virtual environment: Your virtual environment should not be committed to your version control system. It's specific to your local environment and can cause conflicts and unnecessary bloat in your repository.</p> </li> <li> <p>Secure sensitive data: If your Streamlit app uses sensitive data (such as API keys or passwords), ensure that this information is stored securely, ideally in environment variables. Do not include this information directly in your code.</p> </li> </ul> <p>By following these best practices, you can keep your Streamlit projects organized, secure, and free from unnecessary bloat in your version control history.</p>","tags":["Streamlit"]},{"location":"streamlit/Introduction/#resources-and-further-reading","title":"Resources and Further Reading","text":"<p>Here are some valuable resources and references to help you dive deeper into Streamlit and enhance your skills:</p>","tags":["Streamlit"]},{"location":"streamlit/Introduction/#official-streamlit-documentation","title":"Official Streamlit Documentation","text":"<ul> <li> <p>Streamlit Official Documentation: The official documentation is your go-to source for comprehensive information on Streamlit's features, components, and usage. It includes tutorials, guides, and examples to help you get the most out of Streamlit.</p> </li> <li> <p>Streamlit Component API Documentation: Explore the Streamlit Component API documentation to learn how to create and use custom components in your Streamlit apps.</p> </li> </ul>","tags":["Streamlit"]},{"location":"streamlit/Introduction/#community-and-support","title":"Community and Support","text":"<ul> <li> <p>Streamlit Community: Join the Streamlit community forum to connect with other users, ask questions, share your projects, and seek assistance. It's a great place to learn from experienced Streamlit developers.</p> </li> <li> <p>Stack Overflow: Explore Streamlit-related questions and answers on Stack Overflow, a valuable resource for troubleshooting and problem-solving.</p> </li> </ul>","tags":["Streamlit"]},{"location":"streamlit/Introduction/#streamlit-gallery","title":"Streamlit Gallery","text":"<ul> <li>Streamlit Gallery: Explore the Streamlit Gallery to find a collection of example apps, demos, and templates. It's an excellent source of inspiration for your own projects.</li> </ul>","tags":["Streamlit"]},{"location":"streamlit/Introduction/#streamlit-roadmap-app","title":"Streamlit Roadmap App","text":"<ul> <li>Streamlit Roadmap App: Access the Streamlit Roadmap App to stay updated on the latest features, enhancements, and future plans for Streamlit's development.</li> </ul>","tags":["Streamlit"]},{"location":"streamlit/Introduction/#blogs-and-tutorials","title":"Blogs and Tutorials","text":"<ul> <li> <p>Towards Data Science Streamlit Articles: Read blog articles and tutorials related to Streamlit on Towards Data Science. You'll find in-depth guides, tips, and project showcases.</p> </li> <li> <p>Medium Streamlit Stories: Discover Streamlit-related stories and articles on Medium. Many authors share their experiences and insights about using Streamlit.</p> </li> </ul>","tags":["Streamlit"]},{"location":"streamlit/Introduction/#github-repositories","title":"GitHub Repositories","text":"<ul> <li>Streamlit GitHub Repository: Explore the source code for Streamlit on its GitHub repository. You can also contribute to the development of Streamlit if you're interested.</li> </ul> <p>These resources are designed to help you become proficient in Streamlit and explore its capabilities in greater detail. Whether you're a beginner or an experienced developer, there's something for everyone to learn and build with Streamlit.</p>","tags":["Streamlit"]},{"location":"streamlit/deploy/","title":"Deploying a Streamlit Application","text":"<p>In this guide, we'll walk you through the process of deploying a Streamlit application from a GitHub repository to Streamlit Cloud. Streamlit Cloud is a platform that makes it easy to share your data apps with the world. Follow these steps to get your Streamlit app up and running in the cloud.</p>","tags":["Streamlit","Streamlit Cloud"]},{"location":"streamlit/deploy/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have the following prerequisites in place:</p> <ol> <li>A Streamlit application hosted on a GitHub repository.</li> <li>A Streamlit Cloud account. You can sign up for one at Streamlit Cloud.</li> </ol>","tags":["Streamlit","Streamlit Cloud"]},{"location":"streamlit/deploy/#instruction","title":"Instruction","text":"","tags":["Streamlit","Streamlit Cloud"]},{"location":"streamlit/deploy/#step-1-create-a-streamlit-cloud-account","title":"Step 1: Create a Streamlit Cloud Account","text":"<p>If you haven't already, sign up for a Streamlit Cloud account using your preferred authentication method.</p>","tags":["Streamlit","Streamlit Cloud"]},{"location":"streamlit/deploy/#step-2-login-to-streamlit-cloud","title":"Step 2: Login to Streamlit Cloud","text":"<p>Log in to your Streamlit Cloud account to access the Streamlit Cloud dashboard.</p>","tags":["Streamlit","Streamlit Cloud"]},{"location":"streamlit/deploy/#step-3-create-a-new-app","title":"Step 3: Create a New App","text":"<ol> <li>In the Streamlit Cloud dashboard, click the \"Create a New App\" button.</li> <li>Choose a name for your app.</li> <li>Connect your GitHub repository by providing the GitHub repository URL.</li> </ol>","tags":["Streamlit","Streamlit Cloud"]},{"location":"streamlit/deploy/#step-4-configure-deployment-settings","title":"Step 4: Configure Deployment Settings","text":"<p>Streamlit Cloud provides you with various deployment settings, such as the app name, environment variables, and version control options. Configure these settings as per your project requirements.</p> <p></p>","tags":["Streamlit","Streamlit Cloud"]},{"location":"streamlit/deploy/#step-5-deploy-the-app","title":"Step 5: Deploy the App","text":"<p>Click the \"Deploy\" button to start the deployment process. Streamlit Cloud will clone your GitHub repository, install the necessary dependencies, and launch your Streamlit app.</p>","tags":["Streamlit","Streamlit Cloud"]},{"location":"streamlit/deploy/#step-6-access-your-deployed-app","title":"Step 6: Access Your Deployed App","text":"<p>Once the deployment is complete, you'll receive a unique URL for your Streamlit app. You can share this URL with others to access and interact with your application.</p>","tags":["Streamlit","Streamlit Cloud"]},{"location":"streamlit/deploy/#step-7-monitor-and-update","title":"Step 7: Monitor and Update","text":"<p>Streamlit Cloud allows you to monitor your deployed app's usage and performance. You can also easily update your app by pushing changes to your GitHub repository. Streamlit Cloud will automatically rebuild and redeploy your app with the latest changes.</p>","tags":["Streamlit","Streamlit Cloud"]},{"location":"streamlit/deploy/#conclusion","title":"Conclusion","text":"<p>Congratulations! Your Streamlit application is now live on Streamlit Cloud and accessible to a global audience.</p> <p>For more advanced configuration options and detailed instructions, refer to the official Streamlit Cloud documentation.</p> <p>Enjoy sharing your Streamlit apps with the world!</p>","tags":["Streamlit","Streamlit Cloud"]},{"location":"sw/","title":"Software Installation","text":""},{"location":"sw/#introduction","title":"Introduction","text":"<p>Welcome to the comprehensive guide for setting up your development environment and tools. Whether you're embarking on your journey into Python programming, version control with Git and GitHub, exploring the power of cloud computing, or seeking an efficient development setup, this document is your go-to resource. We'll walk you through the installation of essential software, introduce you to time-saving extensions and shortcuts, and guide you in setting up cloud accounts for your academic or professional projects. Let's get started on this exciting path of learning, coding, and innovation.</p> <ol> <li> <p>Python Development</p> <ul> <li>Visual Studio Code</li> <li>PyCharm Community Edition</li> <li>Recommended Extensions and Shortcuts</li> </ul> </li> <li> <p>Python Installation</p> <ul> <li>Installing Python 3.x (preferably version 3.7+)</li> <li>Python PATH Configuration for Windows, macOS, and Linux</li> </ul> </li> <li> <p>GitHub Setup</p> <ul> <li>Creating a GitHub Account</li> <li>Installing GitHub Desktop</li> <li>Basic Git Commands</li> <li>Getting Started with GitHub Desktop</li> </ul> </li> <li> <p>WSL (Windows Subsystem for Linux) Installation</p> <ul> <li>Installing Ubuntu within WSL</li> <li>Setting Up Ubuntu in WSL</li> </ul> </li> <li> <p>Cloud Account Setup</p> <ul> <li>Public Cloud Providers (AWS, GCP, Azure)</li> <li>Pro Tips for Maximizing Credits</li> <li>Responsibility for Cloud Billing</li> </ul> </li> </ol>"},{"location":"sw/cloud/","title":"Public Cloud Accounts","text":"<p>In this section, we'll guide you through the process of creating cloud accounts with popular public cloud providers. While you're free to choose any cloud provider, we recommend starting with Google Cloud Platform (GCP) for its generous $300 credits for 3 months. We'll also provide information on Amazon Web Services (AWS) and Microsoft Azure.</p>","tags":["Cloud","Installation"]},{"location":"sw/cloud/#google-cloud-platform-gcp","title":"Google Cloud Platform (GCP)","text":"<ol> <li> <p>Sign Up for GCP: Visit the Google Cloud Platform website and sign up for an account.</p> </li> <li> <p>$300 Credits: GCP offers $300 in credits for the first three months, which should be sufficient for this course. Make the most of these credits as you learn cloud computing.</p> </li> </ol>","tags":["Cloud","Installation"]},{"location":"sw/cloud/#amazon-web-services-aws","title":"Amazon Web Services (AWS)","text":"<ol> <li> <p>Sign Up for AWS: Go to the Amazon Web Services website and sign up for an AWS account.</p> </li> <li> <p>AWS Free Tier: AWS provides a free tier for 12 months, allowing you to explore various services with certain restrictions on resource types. Take advantage of this opportunity to learn and experiment.</p> </li> </ol>","tags":["Cloud","Installation"]},{"location":"sw/cloud/#microsoft-azure","title":"Microsoft Azure","text":"<p>You can also explore Microsoft Azure by signing up on the Microsoft Azure website. While specific offers and credits may vary, Azure provides free trials and various student benefits.</p>","tags":["Cloud","Installation"]},{"location":"sw/cloud/#pro-tip","title":"Pro Tip","text":"Tip <p>To maximize your cloud credits, consider having one team member create a cloud account at a time. By doing this, you can collectively accumulate up to $900 in credits, which can be highly valuable for your cloud-related projects and coursework.</p>","tags":["Cloud","Installation"]},{"location":"sw/cloud/#note","title":"Note","text":"Danger <p>It's important to note that you are responsible for your cloud billing, and the university does not provide any cloud credits. Be mindful of your usage to avoid unexpected charges and ensure responsible cloud computing practices.</p> <p>Enjoy your cloud computing journey!</p>","tags":["Cloud","Installation"]},{"location":"sw/github/","title":"GitHub Account and GitHub Desktop","text":"<p>In this section, you'll learn how to set up a GitHub account and install GitHub Desktop, which will be essential for managing and submitting your assignments using Git and GitHub repositories.</p>","tags":["Github","Gtihub Desktop","Installation"]},{"location":"sw/github/#sign-up-for-a-github-account","title":"Sign Up for a GitHub Account","text":"<p>If you don't already have a GitHub account, follow these steps to sign up:</p> <ol> <li>Sign Up: Visit the GitHub website at github.com.</li> <li>Create an Account: Click on the \"Sign up\" button and follow the on-screen instructions to create your GitHub account.</li> <li>Choose a Plan: Select a plan that suits your needs. GitHub offers free plans for public repositories and provides options for private repositories if needed.</li> </ol>","tags":["Github","Gtihub Desktop","Installation"]},{"location":"sw/github/#install-github-desktop","title":"Install GitHub Desktop","text":"<p>GitHub Desktop is a user-friendly application that simplifies the process of working with Git and GitHub repositories. Here's how to install it:</p> <ol> <li>Download GitHub Desktop: Visit the GitHub Desktop download page at desktop.github.com.</li> <li>Download and Install:<ul> <li>On Windows: Run the downloaded installer and follow the installation instructions.</li> <li>On macOS: Open the downloaded DMG file, drag the GitHub Desktop application into the Applications folder.</li> <li>On Linux: GitHub Desktop is not officially supported on Linux, but you can use Git on the command line or explore alternative Git GUI tools.</li> </ul> </li> </ol>","tags":["Github","Gtihub Desktop","Installation"]},{"location":"sw/github/#basic-git-commands","title":"Basic Git Commands","text":"<p>To get comfortable with using Git for version control, familiarize yourself with some basic Git commands. You can find comprehensive documentation on Git commands in the official Git documentation.</p> Tip <p>Windows users should clone repositories within the WSL file system, typically located at <code>\\\\wsl.localhost\\Ubuntu\\username</code>.</p> <p>Here are a few essential Git commands to get you started:</p> <ul> <li><code>git init</code>: Initialize a new Git repository in a directory.</li> <li><code>git clone [repository URL]</code>: Clone a remote repository to your local machine.</li> <li><code>git add [file]</code>: Stage changes for commit.</li> <li><code>git commit -m \"Your commit message\"</code>: Commit staged changes.</li> <li><code>git push</code>: Push changes to a remote repository.</li> <li><code>git pull</code>: Pull changes from a remote repository.</li> <li><code>git status</code>: Check the status of your repository.</li> <li><code>git log</code>: View commit history.</li> </ul>","tags":["Github","Gtihub Desktop","Installation"]},{"location":"sw/github/#getting-started-with-github-desktop","title":"Getting Started with GitHub Desktop","text":"<p>GitHub Desktop simplifies common Git and GitHub tasks through a graphical interface. Refer to the official GitHub Desktop documentation to learn how to use the application effectively.</p> <p>With a GitHub account and GitHub Desktop installed, you'll be ready to create, manage, and submit your assignments as GitHub repository links.</p> <p>Happy coding and version control with Git and GitHub!</p>","tags":["Github","Gtihub Desktop","Installation"]},{"location":"sw/ide/","title":"Integrated Development Environment (IDE)","text":"<p>In this guide, you will find step-by-step instructions for installing two popular Python Integrated Development Environments (IDEs): Visual Studio Code and PyCharm Community Edition. Additionally, we will explore some extensions and shortcuts to boost your productivity.</p>","tags":["IDE","Installation"]},{"location":"sw/ide/#visual-studio-code","title":"Visual Studio Code","text":"","tags":["IDE","Installation"]},{"location":"sw/ide/#installation-steps","title":"Installation Steps","text":"<ol> <li> <p>Download: Visit the Visual Studio Code website and download the installer for your operating system (Windows, macOS, or Linux).</p> </li> <li> <p>Installation:</p> </li> <li>On Windows: Run the installer and follow the on-screen instructions.</li> <li>On macOS: Open the downloaded DMG file, and drag the Visual Studio Code application into the Applications folder.</li> <li> <p>On Linux: Follow the instructions for your specific distribution.</p> </li> <li> <p>Launch VS Code: After installation, open Visual Studio Code.</p> </li> </ol>","tags":["IDE","Installation"]},{"location":"sw/ide/#productivity-enhancements","title":"Productivity Enhancements","text":"<p>Visual Studio Code offers a wide range of extensions and shortcuts to enhance your Python development experience. Here are a few recommendations:</p> <ul> <li> <p>Python Extension: Search for and install the \"Python\" extension by Microsoft to enable Python support.</p> </li> <li> <p>Code Runner: Install the \"Code Runner\" extension to execute Python code directly within VS Code.</p> </li> <li> <p>Visual Studio IntelliCode: This extension uses AI to suggest code completions, helping you write Python code more efficiently.</p> </li> <li> <p>Live Server: If you work with web development, consider installing the \"Live Server\" extension for live preview and auto-reload of web pages.</p> </li> <li> <p>GitHub Copilot: If available, install the \"GitHub Copilot\" extension for AI-powered code suggestions and assistance.</p> </li> <li> <p>Remote SSH: If you need to work on remote servers, the \"Remote - SSH\" extension can help you connect to remote machines for Python development.</p> </li> </ul>","tags":["IDE","Installation"]},{"location":"sw/ide/#pycharm-community-edition","title":"PyCharm Community Edition","text":"","tags":["IDE","Installation"]},{"location":"sw/ide/#installation-steps_1","title":"Installation Steps","text":"<ol> <li> <p>Download: Go to the PyCharm Community Edition download page and download the installer for your operating system.</p> </li> <li> <p>Installation:</p> </li> <li>On Windows: Run the installer and follow the on-screen instructions.</li> <li>On macOS: Open the downloaded DMG file, and drag the PyCharm application into the Applications folder.</li> <li> <p>On Linux: Follow the instructions for your specific distribution.</p> </li> <li> <p>Launch PyCharm: After installation, open PyCharm Community Edition.</p> </li> </ol>","tags":["IDE","Installation"]},{"location":"sw/ide/#productivity-enhancements_1","title":"Productivity Enhancements","text":"<p>PyCharm comes with powerful built-in features for Python development. Here are some tips for increased productivity:</p> <ul> <li> <p>Python Interpreter: Configure your Python interpreter in PyCharm to work with your project.</p> </li> <li> <p>Code Inspection: PyCharm offers code inspection and suggestions to improve your code quality. Pay attention to the highlighted sections.</p> </li> <li> <p>Refactoring: Utilize PyCharm's refactoring tools to improve your code structure and maintainability.</p> </li> </ul>","tags":["IDE","Installation"]},{"location":"sw/ide/#conclusion","title":"Conclusion","text":"<p>By following these installation steps and exploring productivity-enhancing extensions and shortcuts, you'll be well-equipped for Python development using Visual Studio Code or PyCharm Community Edition.</p> <p>Good luck with your Python programming journey!</p>","tags":["IDE","Installation"]},{"location":"sw/python/","title":"Python","text":"<p>In this guide, you will find step-by-step instructions for installing Python (version 3.7 or higher) and configuring the Python PATH on different operating systems.</p>","tags":["Python","Installation"]},{"location":"sw/python/#python-installation","title":"Python Installation","text":"","tags":["Python","Installation"]},{"location":"sw/python/#download-and-install-python","title":"Download and Install Python","text":"<ol> <li> <p>Download Python: Visit the official Python website at python.org. We recommend avoiding the latest release and selecting a version of Python 3.7 or higher.</p> </li> <li> <p>Installation for Windows:</p> <ol> <li>Run the downloaded Python installer.</li> <li>During installation, make sure to check the box that says \"Add Python to PATH.\"</li> </ol> </li> <li> <p>Installation for macOS:</p> <ol> <li>Run the downloaded Python installer.</li> <li>Follow the on-screen instructions, ensuring that Python is installed.</li> </ol> </li> <li> <p>Installation for Linux:</p> <ol> <li>Python is often pre-installed on Linux distributions. You can check the version by running <code>python3 --version</code> in the terminal. If Python is not installed, use your distribution's package manager to install it.</li> </ol> </li> </ol>","tags":["Python","Installation"]},{"location":"sw/python/#python-path-configuration","title":"Python PATH Configuration","text":"","tags":["Python","Installation"]},{"location":"sw/python/#macos-and-linux","title":"macOS and Linux","text":"<p>Please refer to video tutorials or documentation specific to your distribution for setting up the Python PATH. The process can vary based on your system's configuration. </p> Tip <p> Python Tutorial: How to Set the Path and Switch Between Different Versions/Executables (Mac &amp; Linux)</p>","tags":["Python","Installation"]},{"location":"sw/python/#windows","title":"Windows","text":"<p>On Windows, the Python installer can add Python to the PATH for you. However, if it's not added during installation or if you need to do it manually, follow these steps:</p> <ol> <li> <p>Search for \"Environment Variables\": In the Windows search bar, type \"Environment Variables\" and select \"Edit the system environment variables.\"</p> </li> <li> <p>System Properties: Click the \"Environment Variables\" button in the \"System Properties\" window.</p> </li> <li> <p>Edit User Variables: In the \"User variables\" section, select \"Path\" and click the \"Edit\" button.</p> </li> <li> <p>Add Python to Path: Click \"New\" and add the path to your Python installation. By default, it's something like <code>C:\\Users\\&lt;YourUsername&gt;\\AppData\\Local\\Programs\\Python\\Python3x</code> (replace <code>&lt;YourUsername&gt;</code> with your actual username and <code>Python3x</code> with the Python version).</p> </li> <li> <p>Confirm Configuration: Close all windows and open a new Command Prompt. You should be able to run Python by typing <code>python</code> or <code>python3</code> in the command prompt.</p> </li> </ol> Tip <p> Python Tutorial: How to Set the Path and Switch Between Different Versions/Executables (Windows)</p>","tags":["Python","Installation"]},{"location":"sw/python/#conclusion","title":"Conclusion","text":"<p>By following these installation and Python PATH configuration steps, you will have Python (version 3.7 or higher) installed on your system. Make sure to use the appropriate instructions for your specific operating system.</p> <p>Happy coding with Python!</p>","tags":["Python","Installation"]},{"location":"sw/wsl/","title":"Windows Subsystem for Linux (WSL) on Windows","text":"<p>When working on cloud-based projects or needing a Linux-like environment on Windows, Windows Subsystem for Linux (WSL) is an excellent solution. It allows you to run a Linux distribution alongside your Windows environment. In this section, you will learn how to install an LTS version of Ubuntu within WSL.</p>","tags":["WSL","Installation"]},{"location":"sw/wsl/#install-linux-on-windows-with-wsl","title":"Install Linux on Windows with WSL","text":"<p>Follow these steps to install Ubuntu on Windows with WSL:</p> <ol> <li> <p>Open Microsoft Store: Search for \"Microsoft Store\" in your Windows search bar and open it.</p> </li> <li> <p>Search for Ubuntu: In the Microsoft Store, search for \"Ubuntu.\" Look for an LTS (Long Term Support) version of Ubuntu.</p> </li> <li> <p>Install Ubuntu: Click on the Ubuntu version you prefer and click the \"Install\" button. The installation process will begin.</p> </li> <li> <p>Initial Configuration: After installation, launch the Ubuntu app.</p> </li> </ol>","tags":["WSL","Installation"]},{"location":"sw/wsl/#setting-up-ubuntu-in-wsl","title":"Setting up Ubuntu in WSL","text":"<p>Once you've installed Ubuntu within WSL, you'll need to perform some initial configuration, including setting a Linux username and password.</p> <ol> <li> <p>Launch Ubuntu: Open the newly installed Ubuntu app from your Windows Start menu.</p> </li> <li> <p>Initial Setup: The first time you run Ubuntu, it will perform some one-time setup, including configuring the user account.</p> </li> <li> <p>Create a Username: You will be prompted to create a Linux username. Enter the desired username.</p> </li> <li> <p>Set a Password: After creating the username, you'll be asked to set a password for your Linux user.</p> </li> <li> <p>Accessing the Terminal: Once the setup is complete, you'll have access to a Linux terminal within WSL, where you can execute Linux commands and install additional packages.</p> </li> </ol>","tags":["WSL","Installation"]},{"location":"sw/wsl/#common-issue","title":"Common Issue","text":"Warning <p>Common installation error: \"Virtualization not enabled.\" Consult your laptop's manufacturer guide for BIOS settings to enable it based on the laptop's make and model.</p>","tags":["WSL","Installation"]},{"location":"sw/wsl/#conclusion","title":"Conclusion","text":"<p>With WSL and Ubuntu installed, you can enjoy a Linux-like environment on your Windows machine, making it easier to work on cloud-based or Linux-based projects. You can use the terminal within WSL for various tasks and development activities.</p> <p>Happy Linux-on-Windows coding!</p>","tags":["WSL","Installation"]},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/llm/","title":"LLM","text":""},{"location":"blog/category/vector-database/","title":"Vector Database","text":""},{"location":"blog/category/pinecone/","title":"Pinecone","text":""},{"location":"blog/category/sec-form/","title":"SEC Form","text":""},{"location":"blog/category/web-scraping/","title":"Web Scraping","text":""}]}